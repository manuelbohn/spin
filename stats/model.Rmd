---
title: "Model"
output: html_document
---

# Model utilities

```{r}
library(tidyverse)
library(rwebppl)
library(brms)
library(coda)
library(ggthemes)
library(readxl)
library(ggpubr)
library(stringr)
library(matrixStats)
library(data.table)
library(bigmemory)
library(langcog)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}


hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}

me_data <- read_csv("../data/me.csv")

data_comb_pre <- read_csv("../data/combination.csv") 

aoa_ratings_all <- read_xlsx(path = "../data/words_aoa_ratings.xlsx", sheet = 1)

aoa_ratings <- aoa_ratings_all %>%
  filter(Word %in% data_comb_pre$item )%>%
  mutate(mean_aoa = as.numeric(Rating.Mean),
         item = Word)%>%
  select(item,mean_aoa)

data_comb <- data_comb_pre%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa), 
         source = "Data")


prior_data <- read_csv("../data/novelty.csv")

prior_model_data <- read_csv("../data/novelty.csv")

me_model_data <- read_csv("../data/me.csv")

data_comb_plot<-data_comb%>%
  mutate(model = "data")

```

```{r rsaUtils}
rsaUtils <- '
var all_objects = [
{ shape: "novel_object"},  
{ shape: "familiar_object"}
]

var labels = ["novel_word","familiar_word"]


var lexicon1 = function(utterance, obj, sem_knowledge){
  utterance.label == "novel_word" ? obj.shape == "novel_object" :
  utterance.label == "familiar_word" ? flip(sem_knowledge) ? 
    obj.shape == "familiar_object" :  
  flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" : 
  true
}

var lexicon2 = function(utterance, obj, sem_knowledge){
  utterance.label == "novel_word" ? obj.shape == "familiar_object" :
  utterance.label == "familiar_word" ? flip(sem_knowledge) ? 
    obj.shape == "familiar_object" : 
  flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" : 
  true
}

var lexiconObjects = {
"novel_word = novel_object": {
novel_object: "novel_word", familiar_object: "familiar_word"
},
"novel_word = familiar_object": {
novel_object: "familiar_word", familiar_object: "familiar_word"
},
}

var lexiconObject = {
"novel_word = novel_object": lexicon1,
"novel_word = familiar_object" : lexicon2
}

var utterancePrior = function(){ return uniformDraw([ {label: "novel_word"}, {label: "familiar_word"}]) }

var LexiconPrior = Categorical({vs: ["novel_word = novel_object","novel_word = familiar_object" ], ps: [1, 1]})

var addNoise = function(dist, noiseParam){
   Infer({model: function(){ 
      return flip(noiseParam) ? uniformDraw([0, 1]) : sample(dist)
    }
   })
}


var foreach = function(fn, lst) {
    var foreach_ = function(i) {
        if (i < lst.length) {
            fn(lst[i]);
            foreach_(i + 1);
        }
    };
    foreach_(0);
};

var logistic = function(x) {1 / (1 + Math.exp(-x))}

var levels = function(df, label){
  return _.uniq(_.map(df, label));
}
'
```

```{r rsa model}
rsaModel <- '
var literalListener = cache(function(utterance, priorProbs, sem_knowledge){
  Infer({method: "enumerate", model: function(){
    var lexiconName = sample(LexiconPrior); 
    var lexicon = lexiconObject[lexiconName];
    var obj = sample( Categorical({vs: all_objects, ps: [.5,.5]}));
    if ("label" in utterance) {
      var truthValue = lexicon(utterance, obj, sem_knowledge);
      condition(truthValue)
    }
    return obj.shape 
  }})
}, 10000)

var speaker = cache(function(obj, lexiconName, priorProbs, speakerOptimality, sem_knowledge){
  Infer({method: "enumerate", model: function(){
    var utterance = utterancePrior();
    var L0 = literalListener(utterance, priorProbs, sem_knowledge);
    factor(speakerOptimality * L0.score(obj.shape))
    return utterance
  }})
}, 10000)

var pragmaticListener = cache(function(utterance, priorProbs, speakerOptimality, sem_knowledge){
  Infer({method: "enumerate", model: function(){
    var lexiconName = sample(LexiconPrior);
    var obj = sample( Categorical({vs: all_objects, ps: priorProbs}));
    var S1 = speaker(obj, lexiconName, priorProbs, speakerOptimality, sem_knowledge);
    observe(S1, utterance)
    return obj.shape == "novel_object" ? 1 : 0
  }})
}, 10000)

'
```

# Model

## Combination Model

```{r work in progress}
sem_subj_str <- '

var data = dataFromR.data
var priorData = dataFromR.data

var priorProbs = [.5,.5]

var familiars = levels(data, "item")
var subjects = levels(data, "subid")
var priorSubjects = levels(priorData, "subid")

var model  = function(){


//////////////// Prior ////////////////////////

 var prior_slope = uniformDrift({a: -2, b: 2, width: 0.4})
  var prior_int = uniformDrift({a: -2, b: 2, width: 0.4})

  var prior_subject_sigma = uniformDrift({a: 0, b:3, width: 0.2}) 

  var priorSampleSub = function(age){
    return gaussianDrift({ mu: age, sigma: prior_subject_sigma, width: 0.1 })
  }

  foreach(function(subid){
      var priorSubjectData = _.filter(priorData, {subid: subid})

      //display(subjects)

      var subj_age = priorSubjectData[0].age

      var subjectPrior = priorSampleSub(subj_age)

    foreach(function(row){
  
      var priorReg = logistic(prior_int + prior_slope * subjectPrior)

      var prior = [priorReg, 1-priorReg]

      var modelPredictions = Infer({method: "enumerate", model: function(){
      var obj = sample( Categorical({vs: all_objects, ps: prior}));
      return obj.shape == "novel_object" ? 1 : 0
       }})
    
     observe(modelPredictions, row.correct)

    }, priorSubjectData)

  }, priorSubjects)

  query.add(["parameter","parameters", "prior"], [prior_int, prior_slope])
  query.add(["parameter","sigma", "prior"], ["NA", prior_subject_sigma])



//////////////// Semantic knowledge and speaker optimality ////////////////////////

  var speakerOptimalityParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 4, width: 0.5})
  }

  var globalLineParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 2, width: 0.5})
  }

  var itemVariability = {
    intercept: uniformDrift({a: 0, b: 2, width: 0.2}),
    slope: uniformDrift({a: 0, b: 1, width: 0.2})
  }

  // conceptually it might not make sense to have two sigmas, but maybe it does
  // intercept sigma probably will be smaller than slope sigma
  // var item_int_sigma = uniformDrift({a: 0, b: 3, width: 0.2})
  
  var sampleItemParameters = function(itemName) { 
    return [itemName, {
        intercept: gaussianDrift({
          mu: globalLineParameters.intercept, 
          sigma: itemVariability.intercept, 
          width: 0.5
        }),
        slope: gaussianDrift({
          mu: globalLineParameters.slope, 
          sigma: itemVariability.slope, 
          width: 0.5
        })
      }]
  }

  // {papaya: {int, slope}, bread: {int, slope}, ... }
  var all_item_parameters = _.fromPairs(map(sampleItemParameters, familiars))
  
  // want something that favors smaller values
  var subject_sigma = uniformDrift({a: 0, b:1, width: 0.1}) 
  
  var sampleLinguisticCompetence = function(age){
    return gaussianDrift({ mu: age, sigma: subject_sigma, width: 0.1 })
  }

  foreach(function(subid){
      var subjectData = _.filter(data, {subid: subid})
      var subj_age = subjectData[0].age
      var subj_linguistic_competence = sampleLinguisticCompetence(subj_age)
      
     // display("age = " + subj_age + " ... intercept = " + subj_linguistic_competence)
  
      // question: should we use age here instead of the random intercept? or a different random intercept?
    
  var speakerOptimality = speakerOptimalityParameters.intercept  + speakerOptimalityParameters.slope * subj_age
      
      // each row is a different item
      foreach(function(row){

        var itemLineParameters = all_item_parameters[row.item]
        
        var sem_knowledge = logistic(itemLineParameters.intercept + 
            itemLineParameters.slope * subj_linguistic_competence)
  
        var modelPredictions = pragmaticListener({label: "novel_word"}, 
              priorProbs, speakerOptimality, sem_knowledge)
        // display(modelPredictions.score(row.correct))
        
        observe(modelPredictions, row.correct)
        
        // if you want to save predictions, make sure to record subid because these predictions are for this particular subid
        // query.add(["modelPrediction", cndtn, age], [age, Math.exp(modelPredictions.score(1))])

      }, subjectData)
    
    // comment out to save memory
  //  query.add(["parameter","subj_intercept", subid], [subjectData[0].age, subj_linguistic_competence])
      
  }, subjects)


//////////////// Model predictions and combination ////////////////////////


var noise = uniformDrift({a: 0, b:1, width: 0.1})
 
var all_log_likes = map(function(row) { 
  
  var priorReg = logistic(prior_int + prior_slope * row.age)

  var cond = row.condition

  var prior = function(cnd){
    var cndPrior = cnd == "congruent" ? [priorReg, 1-priorReg] : [1-priorReg, priorReg] ;
    return cndPrior
  }

  var priorComb = prior(cond)

  var itemLineParameters = all_item_parameters[row.item]

  var sem_knowledge = logistic(itemLineParameters.intercept + 
                               itemLineParameters.slope * row.age)

  var global_sem_knowledge = logistic(globalLineParameters.intercept + 
                               globalLineParameters.slope * row.age)

  var speakerOptimality = speakerOptimalityParameters.intercept  + 
      speakerOptimalityParameters.slope * row.age

  var pragModelPredictions = pragmaticListener({label: "novel_word"}, 
                                               priorComb, speakerOptimality, 
                                               sem_knowledge)

  var noisyPragModelPredictions = addNoise(pragModelPredictions, noise)
  


  var globalPragModelPredictions = pragmaticListener({label: "novel_word"}, 
                                               priorComb, speakerOptimality, 
                                               global_sem_knowledge)

   var noisyGlobalPragModelPredictions = addNoise(globalPragModelPredictions, noise)
  

   var flatModelPredictions = pragmaticListener({label: "novel_word"}, 
                                               [.5, .5], speakerOptimality, 
                                               sem_knowledge)

   var noisyFlatModelPredictions = addNoise(flatModelPredictions, noise)

  
  var priorOnlyModelPredictions = Infer({method: "enumerate", model: function(){
      var obj = sample( Categorical({vs: all_objects, ps: priorComb}));
      return obj.shape == "novel_object" ? 1 : 0
       }})
   
  var noisyPriorOnlyModelPredictions = addNoise(priorOnlyModelPredictions, noise) 

  query.add(["modelPrediction","pragmatic","free", row.alignment, row.item, row.subid], [row.age, Math.exp(pragModelPredictions.score(1))])  

//  query.add(["modelPrediction","globalPrag","free", row.alignment, row.item, row.subid], [row.age, Math.exp(globalPragModelPredictions.score(1))])  

  return {
    prag: pragModelPredictions.score(row.correct),
    noisePrag: noisyPragModelPredictions.score(row.correct),
    
    globalPrag: globalPragModelPredictions.score(row.correct),
    noiseGlobalPrag: noisyGlobalPragModelPredictions.score(row.correct),
    
    flat: flatModelPredictions.score(row.correct),
    noiseFlat: noisyFlatModelPredictions.score(row.correct),
    
    priorOnly: priorOnlyModelPredictions.score(row.correct),
    noisePriorOnly: noisyPriorOnlyModelPredictions.score(row.correct)
  }

}, dataFromR.comb_data)


var log_likelihood_prag = sum(_.map(all_log_likes, "prag"))
var log_likelihood_prag_noise = sum(_.map(all_log_likes, "noisePrag"))

var log_likelihood_prag_global = sum(_.map(all_log_likes, "globalPrag"))
var log_likelihood_prag_global_noise = sum(_.map(all_log_likes, "noiseGlobalPrag"))

var log_likelihood_flat = sum(_.map(all_log_likes, "flat"))
var log_likelihood_flat_noise = sum(_.map(all_log_likes, "noiseFlat"))

var log_likelihood_prior_only = sum(_.map(all_log_likes, "priorOnly"))
var log_likelihood_prior_only_noise = sum(_.map(all_log_likes, "noisePriorOnly"))

query.add(["log_like", "pragmatics", "parameter_free"], ["NA", log_likelihood_prag])
query.add(["log_like", "pragmatics", "noise"], ["NA", log_likelihood_prag_noise])

query.add(["log_like", "pragmatics_global", "parameter_free"], ["NA", log_likelihood_prag_global])
query.add(["log_like", "pragmatics_global", "noise"], ["NA", log_likelihood_prag_global_noise])

query.add(["log_like", "flat_prior", "parameter_free"], ["NA", log_likelihood_flat])
query.add(["log_like", "flat_prior", "noise"], ["NA", log_likelihood_flat_noise])

query.add(["log_like", "prior_only", "parameter_free"], ["NA", log_likelihood_prior_only])
query.add(["log_like", "prior_only", "noise"], ["NA", log_likelihood_prior_only_noise])


  foreach(function(item){
    var itemLineParameters = all_item_parameters[item]
    query.add(["parameter","items", item], [itemLineParameters.intercept, itemLineParameters.slope])
  }, familiars)

  query.add(["parameter","parameters","speaker_optimality"], [speakerOptimalityParameters.intercept, speakerOptimalityParameters.slope])
  query.add(["parameter","parameters", "global_sem"], [globalLineParameters.intercept, globalLineParameters.slope])
  query.add(["parameter","sigma", "global_sem_sigmas"], [itemVariability.intercept, itemVariability.slope])
  query.add(["parameter","sigma", "global_subject_sigmas"], ["NA", subject_sigma])
 
  return query 
}


var header = "iteration,a,b,c,d,e,f,g,h"

var randomNumberForFile = Math.round(uniform(0, 1000))

var callback = webpplSampleWriter.streamQueryCSV("../saves/model/samples"+randomNumberForFile+".csv", header);

Infer({model,
      samples: 10,
      burn: 10,
      verbose: T,
      lag: 1,
      verboseLag: 1000,
      method: "incrementalMH",
      onlyMAP: true,
      callbacks: [callback]});
'
```

## Model run

```{r}
sem_know_model<- webppl(
  program_code = paste(rsaUtils, rsaModel, sem_subj_str , sep='\n'),
  data = list(data = me_model_data, comb_data = data_comb, prior_data = prior_model_data),
  data_var = "dataFromR",
  model_var = "model",
  chains = 2,
  cores = 2,
  packages = c("webppl_packages/webppl-sample-writer")
)
```


# Ignore below for now

## Model comparison

```{r}
log_like <- bind_rows(
  readRDS("../saves/log_like_chain1.rds")%>%mutate(chain = 1),
  readRDS("../saves/log_like_chain2.rds")%>%mutate(chain = 2),
  readRDS("../saves/log_like_chain3.rds")%>%mutate(chain = 3),
  readRDS("../saves/log_like_chain4.rds")%>%mutate(chain = 4),
  readRDS("../saves/log_like_chain5.rds")%>%mutate(chain = 5)
)

log_like_model<- log_like %>%
  group_by(model,parameter)%>%
  summarize(logP = logSumExp(log_like))%>%
  spread(parameter, logP)

log_like_chain<- log_like %>%
  group_by(chain, model,parameter)%>%
  summarize(logP = logSumExp(log_like))%>%
  spread(parameter, logP)

log_like_chain %>%
  group_by(model)%>%
  mutate(noise = round(noise,2),
         parameter_free = round(parameter_free,2))%>%
  summarise(range_param_free = paste(range(parameter_free)[1],range(parameter_free)[2], sep = " / "),
            range_noise = paste(range(noise)[1],range(noise)[2], sep = " / "))



log_like_2 <-  readRDS("../saves/sem_know_model_variant3_200k_model_comparison_noise.rds")%>%
  separate(Parameter, into = c("parameter", "model","noise","alignment"), sep = ",")%>%
  filter(parameter == "log_like")%>%
  separate(value, into = c("na", "log_like"),sep = ",")%>%
  select(-na, -alignment)%>%
  mutate(log_like =as.numeric(str_remove(log_like,"[)]")))

log_like_model_2 <- log_like_2%>%
  group_by(model,noise)%>%
  summarize(logP = logSumExp(log_like))


model_comp <- log_like_2%>%
  spread(model,log_like)%>%
  mutate(prag_vs_prag_global = pragmatics - pragmatics_global,
         prag_vs_flat_prior = pragmatics - flat_prior,
         prag_vs_prior_only = pragmatics - prior_only)%>%
  group_by(noise)%>%
  summarize(prag_vs_prag_global = logSumExp(prag_vs_prag_global),
            prag_vs_flat_prior = logSumExp(prag_vs_flat_prior),
            prag_vs_prior_only = logSumExp(prag_vs_prior_only))
  


```

```{r}
ggplot(log_like_model, aes(x = model, y = logP, col = model))+
  geom_segment( aes(x=model, xend=model, y=-1150, yend=logP), col = "black", size = 1)+
  geom_point(size = 7)+
  theme_few()+ 
  xlab("")+
  ylab("Log-likelihood")+
  theme(axis.text.x=element_text(angle = 45, vjust = 1, hjust = 1))+
  facet_grid(~parameter)+
  ylim(-1750,-1150)+
  scale_color_ptol(breaks=c("pragmatics","pragmatics_global","prior_only","flat_prior"),
                     labels=c("Pragmatic", "Pragmatic global","Prior only","Flat prior"))+
    theme(axis.text.x=element_blank(), 
        axis.ticks.x=element_blank())
```

```{r}
ggsave("../graphs/like.png", width = 6, height = 2.5, scale = 1.5)
```


## Summarizing parameters

```{r}

model_pred <- read_csv("../saves/model/samples78.csv") %>%
  filter(a == "modelPrediction")%>%
  select(-a)%>%
  rename(model = b, 
         parameter = c,
         alignment = d,
         id = f,
         item = e,
         age = g, 
         pred = h)

saveRDS(model_pred, "../saves/model_pred.rds")
model_pred <- readRDS("../saves/model_pred.rds")

  
model_params <- bind_rows(
  readRDS("../saves/model_params_chain1.rds")%>%mutate(chain = 1),
  readRDS("../saves/model_params_chain2.rds")%>%mutate(chain = 2),
  readRDS("../saves/model_params_chain3.rds")%>%mutate(chain = 3),
  readRDS("../saves/model_params_chain4.rds")%>%mutate(chain = 4),
  readRDS("../saves/model_params_chain5.rds")%>%mutate(chain = 5)
  
)
  
item_params <-  model_params %>%
  filter(b == "items")%>%
  select(-a,-b, -g,-h, -f)%>%
    rename(item = c,
         int = d,
         slope = e)%>%
  mutate(slope = as.numeric(slope),
         int = as.numeric(int))%>%
  gather(parameter, value, int, slope)

global_params <- model_params %>%
  filter(b == "parameters")%>%
  select(-a,-b, -g,-h, -f)%>%
    rename(token = c,
         int = d,
         slope = e)%>%
  mutate(slope = as.numeric(slope),
         int = as.numeric(int))%>%
  gather(parameter, value, int, slope)

global_sigma<- model_params %>%
  filter(b == "sigma")%>%
  select(-a,-b, -g,-h, -f)%>%
    rename(token = c,
         int = d,
         slope = e)%>%
  mutate(slope = as.numeric(slope),
         int = as.numeric(int))%>%
  gather(parameter, value, int, slope)


#### summaries

item_params_summary <- item_params %>%
  group_by(item, parameter)%>%
  summarise(mode = estimate_mode(value),
            uci = hdi_upper(value),
            lci = hdi_lower(value))


global_params_summary <- global_params %>%
  group_by(token, parameter)%>%
  summarise(mode = estimate_mode(value),
            uci = hdi_upper(value),
            lci = hdi_lower(value))


model_pred %>%
  filter(item == "pawn")%>%
  arrange(age)
```
## Plotting model parameters

### Semantic knowledge

```{r}
plot_item_params <- ggplot(item_params, aes(x = value, fill = factor(chain)))+
  geom_density(alpha = 0.3)+
  xlab("Parameter")+
  facet_grid(parameter~item, scales = 'free')+
  theme_few()

plot_item_params
```

```{r}
#ggsave("../graphs/item_params_by_chain.pdf", width = 8, height = 3, scale = 1.5)
```

### Global parameters

```{r}

plot_global_params <- ggplot(global_params, aes(x = value, fill = factor(chain)))+
  geom_density(alpha = 0.3)+
  xlab("Parameter")+
  facet_grid(parameter~token, scales = 'free')+
  theme_few()

plot_global_params
```

### Global sigmas

```{r}
plot_global_sigma <- ggplot(global_sigma, aes(x = value, fill = factor(chain)))+
  geom_density(alpha = 0.3)+
  xlab("Sigma")+
  facet_grid(~parameter, scales = 'free')+
  theme_few()

plot_global_sigma
```


```{r}
plotg_subj_sigma <- ggplot(subj_sigma, aes(x = sigma, fill = factor(Chain)))+
  geom_density(alpha = 0.3)+
  xlab("Subject Sigma")+
  theme_few()

plotg_subj_sigma
```


```{r}

select_id <- me_model_data %>%
  filter(age < 1.3,
         age >1)%>%
  group_by(age,subid)%>%
  summarise(mean = mean(correct))%>%
  mutate(id = subid)


subject_params <- subject_params %>%
  left_join(select_id)%>%
  filter(mean != "NA")%>%
  ungroup()


ggplot(subject_params, aes(x = int, fill = factor(Chain)))+
  geom_density(alpha = 0.3)+
  geom_vline(aes(xintercept = age), size = 1.5)+
  #geom_vline(aes(xintercept = mean), size = 1.5, col = "red")+
  facet_grid(id+ round(mean,2)~.)+
  xlab("Subject Intercept")+
  theme_few()



#ggsave("../graphs/subj_intercept.pdf", width = 5, height = 8, scale = 1.5)

```

## Plotting semantic knowledge

```{r}
aoa_ratings_all <- read_xlsx(path = "../data/words_aoa_ratings.xlsx", sheet = 1)

aoa_ratings <- aoa_ratings_all %>%
  filter(Word %in% item_params_summary$item )%>%
  mutate(mean_aoa = as.numeric(Rating.Mean),
         item = Word)%>%
  select(item,mean_aoa)

plot_sem_know <- item_params_summary%>%
  select(-uci,-lci)%>%
  spread(parameter, -item) %>%
  left_join(expand.grid(
      age = unique(me_model_data$age),
      item = unique(me_model_data$familiar)), .) %>%
  mutate(sem_know = plogis(int + slope * age)) %>%
  left_join(aoa_ratings) %>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa))

ggplot(plot_sem_know, aes(x = age+2, y= sem_know, col = item, group = item))+
  geom_line(size = 1)+
  ylab("Semantic knowledge")+
  xlab("Age")+
  ylim(0,1)+
  theme_few()+
  scale_colour_viridis_d(name = "familiar object")
```

## Plotting model predictions

### Pragmatic free model

```{r}


model_pred_prag_free<- bind_rows(
  readRDS("../saves/model_pred_pragmatic_free_chain1.rds")%>%mutate(chain = 1),
  readRDS("../saves/model_pred_pragmatic_free_chain2.rds")%>%mutate(chain = 2),
  readRDS("../saves/model_pred_pragmatic_free_chain3.rds")%>%mutate(chain = 3),
  readRDS("../saves/model_pred_pragmatic_free_chain4.rds")%>%mutate(chain = 4),
  readRDS("../saves/model_pred_pragmatic_free_chain5.rds")%>%mutate(chain = 5)
)

plot_model_pred_prag_free <- model_pred%>%
  mutate(model = paste(model, parameter, sep = "_"))%>%
  group_by(model, alignment, item, age)%>%
  summarise(mean = estimate_mode(pred),
            ci_lower = hdi_lower(pred),
            ci_upper = hdi_upper(pred))

saveRDS(plot_model_pred_prag_free, "../saves/plot_model_pred_prag_free.rds")
plot_model_pred_prag_free <- readRDS("../saves/plot_model_pred_prag_free.rds")


ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_smooth(data = data_comb_plot, aes(x = age_num, y = correct), col = "black", method = "loess", se = T, alpha = .5, span = 2)+
  geom_line(data = plot_model_pred_prag_free, aes(x=age+2,y = mean, col = model), size = 1)+
  geom_ribbon(data = plot_model_pred_prag_free, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = model), alpha = .4) +
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(alignment~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F, fill = F, col = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)

x <- sample(1:48000, 500)

plot_model_prag_noise_line <- model_pred_prag_free%>%
  filter(iteration %in% x)

ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_line(data = plot_model_prag_noise_line, aes(x=age+2,y = pred, col = factor(chain), group = interaction(chain,iteration)),size = .2, alpha = .05)+
  geom_smooth(data = data_comb_plot, aes(x = age_num, y = correct), col = "black", method = "loess", se = T, alpha = .5, span = 2)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(alignment~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F, fill = F)+ 
  scale_colour_ptol(name = NULL)
```

```{r}
ggsave("../graphs/pred_model_data.pdf", width = 10, height = 4, scale = 1)
```

### Pragmatic noise model

```{r}

model_pred_prag_noise<- bind_rows(
  readRDS("../saves/model_pred_pragmatic_noise_chain1.rds")%>%mutate(chain = 1),
  readRDS("../saves/model_pred_pragmatic_noise_chain2.rds")%>%mutate(chain = 2),
  readRDS("../saves/model_pred_pragmatic_noise_chain3.rds")%>%mutate(chain = 3),
  readRDS("../saves/model_pred_pragmatic_noise_chain4.rds")%>%mutate(chain = 4),
  readRDS("../saves/model_pred_pragmatic_noise_chain5.rds")%>%mutate(chain = 5)
)

plot_model_pred_prag_noise <- model_pred_prag_noise%>%
  mutate(model = paste(model, parameter, sep = "_"))%>%
  group_by(model, alignment, item, age)%>%
  summarise(mean = estimate_mode(pred),
            ci_lower = hdi_lower(pred),
            ci_upper = hdi_upper(pred))


saveRDS(plot_model_pred_prag_noise, "../saves/plot_model_pred_prag_noise.rds")
plot_model_pred_prag_noise <- readRDS("../saves/plot_model_pred_prag_noise.rds")

plot_model_pred_prag_noise_chain <- model_pred_prag_noise%>%
  mutate(model = paste(model, parameter, sep = "_"))%>%
  group_by(model, alignment, item,chain, age)%>%
  summarise(mean = estimate_mode(pred),
            ci_lower = hdi_lower(pred),
            ci_upper = hdi_upper(pred))


saveRDS(plot_model_pred_prag_noise_chain, "../saves/plot_model_pred_prag_noise_chain.rds")
plot_model_pred_prag_noise_chain <- readRDS("../saves/plot_model_pred_prag_noise_chain.rds")

ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
geom_smooth(data = data_comb_plot, aes(x = age_num, y = correct), col = "black", method = "loess", se = T, alpha = .5, span = 2)+
  geom_ribbon(data = plot_model_pred_prag_noise, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = model), alpha = .3) +
  geom_line(data = plot_model_pred_prag_noise, aes(x=age+2,y = mean, col = model), size = 1, alpha = .5)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(alignment~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F, fill = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)

x <- sample(1:48000, 500)

plot_model_prag_noise_line <- model_pred_prag_noise%>%
  filter(iteration %in% x)

ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_smooth(data = data_comb_plot, aes(x = age_num, y = correct), col = "black", method = "loess", se = T, alpha = .4, span = 2)+
  geom_line(data = plot_model_prag_noise_line, aes(x=age+2,y = pred, col = factor(chain), group = interaction(chain,iteration)),size = .2, alpha = .075)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(alignment~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F, fill = F)+ 
  scale_colour_ptol(name = NULL)
```

```{r}
ggsave("../graphs/model_pred_noise_ribbon.pdf", width = 10, height = 4, scale = 1.2)
```

### Global pragmatic free model

```{r}

model_pred_global_free<- bind_rows(
  readRDS("../saves/model_pred_globalPrag_free_chain1.rds")%>%mutate(chain = 1),
  readRDS("../saves/model_pred_globalPrag_free_chain2.rds")%>%mutate(chain = 2)
)

plot_model_pred_global_free <- model_pred_global_free%>%
  mutate(model = paste(model, parameter, sep = "_"))%>%
  group_by(model, alignment, item, age)%>%
  summarise(mean = estimate_mode(pred),
            ci_lower = hdi_lower(pred),
            ci_upper = hdi_upper(pred))


saveRDS(plot_model_pred_global_free, "../saves/plot_model_pred_global_free.rds")
plot_model_pred_global_free <- readRDS("../saves/plot_model_pred_global_free.rds")

ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  #geom_smooth(data = data_comb_plot, aes(x = age_num, y = correct), col = "black", method = "loess", se = T, alpha = .5, span = 2)+
  geom_line(data = plot_model_pred_global_free, aes(x=age+2,y = mean, col = model), size = 1)+
  geom_ribbon(data = plot_model_pred_global_free, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = model), alpha = .4) +
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(model + alignment~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F, fill = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)
```

### Global pragmatic noise model

```{r}

model_pred_global_noise<- bind_rows(
  readRDS("../saves/model_pred_globalPrag_noise_chain1.rds")%>%mutate(chain = 1),
  readRDS("../saves/model_pred_globalPrag_noise_chain2.rds")%>%mutate(chain = 2)
)

plot_model_pred_global_noise <- model_pred_global_noise%>%
  mutate(model = paste(model, parameter, sep = "_"))%>%
  group_by(model, alignment, item, age)%>%
  summarise(mean = estimate_mode(pred),
            ci_lower = hdi_lower(pred),
            ci_upper = hdi_upper(pred))


saveRDS(plot_model_pred_global_noise, "../saves/plot_model_pred_global_noise.rds")
plot_model_pred_global_noise <- readRDS("../saves/plot_model_pred_global_noise.rds")


ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  #geom_smooth(data = data_comb_plot, aes(x = age_num, y = correct), col = "black", method = "loess", se = T, alpha = .5, span = 2)+
  geom_line(data = plot_model_pred_global_noise, aes(x=age+2,y = mean, col = model), size = 1)+
  geom_ribbon(data = plot_model_pred_global_noise, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = model), alpha = .4) +
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(model + alignment~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F, fill = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)
```

## Correlate model predictions and data 

```{r}
binnned_data <- data_comb%>%
  group_by(subage, alignment, item)%>%
  summarize(k = sum(correct), n = n())%>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         data_mean = (a-1)/(a+b-2),
         data_ci_lower  = qbeta(.025, a, b),
         data_ci_upper = qbeta(.975, a, b),
         age = factor(subage))%>%
  select(-a,-b,-n,-k)


cor_prag_free <- model_pred_prag_free%>%
  mutate(model = paste(model, parameter, sep = "_"))%>%
  mutate(age = as.numeric(age) + min(me_data$age_num),
         age = cut(age, 
                      breaks = c(2,3,4,5),
                      labels = c(2,3,4)))%>%
  group_by(model, alignment, item, age)%>%
  summarise(model_mean = estimate_mode(pred),
            model_ci_lower = hdi_lower(pred),
            model_ci_upper = hdi_upper(pred))

saveRDS(cor_prag_free, "../saves/cor_prag_free.rds")
cor_prag_free <- readRDS( "../saves/cor_prag_free.rds")

cor_prag_noise <- model_pred_prag_noise%>%
  mutate(model = paste(model, parameter, sep = "_"))%>%
  mutate(age = as.numeric(age) + min(me_data$age_num),
         age = cut(age, 
                      breaks = c(2,3,4,5),
                      labels = c(2,3,4)))%>%
  group_by(model, alignment, item, age)%>%
  summarise(model_mean = estimate_mode(pred),
            model_ci_lower = hdi_lower(pred),
            model_ci_upper = hdi_upper(pred))

saveRDS(cor_prag_noise, "../saves/cor_prag_noise.rds")
cor_prag_noise <- readRDS( "../saves/cor_prag_noise.rds")

cor_global_free <- model_pred_global_free%>%
  mutate(model = paste(model, parameter, sep = "_"))%>%
  mutate(age = as.numeric(age) + min(me_data$age_num),
         age = cut(age, 
                      breaks = c(2,3,4,5),
                      labels = c(2,3,4)))%>%
  group_by(model, alignment, item, age)%>%
  summarise(model_mean = estimate_mode(pred),
            model_ci_lower = hdi_lower(pred),
            model_ci_upper = hdi_upper(pred))

cor_global_noise <- model_global_noise%>%
  mutate(model = paste(model, parameter, sep = "_"))%>%
  mutate(age = as.numeric(age) + min(me_data$age_num),
         age = cut(age, 
                      breaks = c(2,3,4,5),
                      labels = c(2,3,4)))%>%
  group_by(model, alignment, item, age)%>%
  summarise(model_mean = estimate_mode(pred),
            model_ci_lower = hdi_lower(pred),
            model_ci_upper = hdi_upper(pred))

cor_model_pred <- bind_rows(
  cor_prag_free,
  cor_prag_noise
 # cor_global_free,
#  cor_global_noise
)


plot_cor_model_pred <- cor_model_pred %>%
  left_join(
    bind_rows(
      #binnned_data%>%mutate(model = "globalPrag_free"),
      #binnned_data%>%mutate(model = "globalPrag_noise"),
      binnned_data%>%mutate(model = "pragmatic_free"),
      binnned_data%>%mutate(model = "pragmatic_noise")
      )
  )

saveRDS(plot_cor_model_pred, "../saves/model_cor.rds")
plot_cor_model_pred <- readRDS( "../saves/model_cor.rds")


ggplot(data = plot_cor_model_pred,aes(x = model_mean, y = data_mean, col = item)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_errorbar(aes(ymin = data_ci_lower, ymax = data_ci_upper),width = 0,size = .7)+
  geom_errorbarh(aes(xmin = model_ci_lower, xmax = model_ci_upper), height = 0,size = .7)+
  geom_point(size = 1.5, stroke = 1)+
  coord_fixed()+
 stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = model_mean, y = data_mean), inherit.aes = F, size = 3)+
  xlim(0,1)+ylim(0,1)+
  xlab("Model")+
  ylab("Data")+
  facet_grid(model~age)+
  theme_few() + 
  scale_colour_viridis_d(name ="Item")
```

```{r}
ggsave("../graphs/cor_model_data.pdf", width = 8, height = 3.5, scale = 1)
```


# MAP based inverse predictions for ME data

```{r}
# speak_opt_params <- speak_opt %>%
#   group_by(parameter)%>%
#   summarise(mode = estimate_mode(value),
#             uci = hdi_upper(value),
#             lci = hdi_lower(value))
# 

inv_model_data <- item_params_summary%>%
  select(-uci,-lci)%>%
  spread(parameter, mode)%>%
  tidyr::expand(item, int,slope,age = unique(me_model_data$age- min(me_model_data$age)))%>%
  mutate(so_int = global_params_summary%>%filter(token == "speaker_optimality", parameter == "int")%>%pull(mode),
         so_slope = global_params_summary%>%filter(token == "speaker_optimality", parameter == "slope")%>%pull(mode))%>%
  mutate(speaker_optimality = so_int + so_slope * age,
         semantic_knowledge = plogis(int + slope * age))

```

```{r}
invPred <- '
var allData = dataFromR.data

var priorProbs = [.5,.5]

var output = map(function(row){

    var modelPredictions = pragmaticListener({label: "novel_word"}, priorProbs, row.speaker_optimality,row.semantic_knowledge)

    return extend([row.item + "/" + row.age, Math.exp(modelPredictions.score(1))])

}, allData)

output


'
```


```{r predictions pragmatic model ex 3 adults}
inv_model_pred<- webppl(
  program_code = paste(rsaUtils, rsaModel, invPred , sep='\n'),
  data =list(data = inv_model_data),
  data_var = "dataFromR"
)
# 
# #saveRDS(inv_model_pred, "../saves/inv_model_pred.rds")
# #inv_model_pred <- readRDS("../saves/inv_model_pred.rds")
# 
inv_predictions <- inv_model_pred %>%
  separate(`0`, into = c("item", "age"), sep="/")%>%
  mutate(age = as.numeric(age))%>%
  dplyr::rename( me_effect = `1`)%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         age = age + min(me_data$age_num))
```

```{r}
# plot_model_pred <- model_pred_summary%>%
#   left_join(aoa_ratings) %>%
#   ungroup()%>%
#   mutate(item = fct_reorder(factor(item), mean_aoa))

model_plot <- ggplot(inv_predictions, aes(x=age,y = me_effect, col = item))+
  geom_hline(yintercept = 0.5, lty=2)+
  geom_line(size = 1)+
  ggtitle("Inverse Model Predictions")+
  labs(x="Age",y="Proportion correct")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d()


model_plot
```

```{r}
p2 <- read_csv("../data/me.csv")%>%
  filter(trial != "train1",
         trial != "train2")%>%
  mutate(item = familiar)%>%
    left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(familiar = fct_reorder(factor(item), mean_aoa))

data_plot <- ggplot(data = p2, aes(x = age_num, y = correct, col = familiar, fill = familiar)) +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_jitter(width = 0, height = 0.05, alpha = .6)+
  geom_smooth(method = "loess", se = F, alpha = .5, span = 2)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  #facet_grid(~familiar)+
  ggtitle("Data")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d(name = "familiar object")+
  scale_fill_viridis_d(name = "familiar object")

```

```{r}
ggarrange(data_plot,model_plot, common.legend = T, legend = "right", nrow = 1, ncol = 2)
```

## For comparison: GLMM for ME effect and data

```{r}
me_data <- read_csv("../data/me.csv")%>%
    filter(trial != "train1",
           trial != "train2")%>%
  mutate(z_age = age_num - mean(age_num))

# me_model <- brm(correct ~ z_age + (1|subid) + (z_age | familiar),
#                 data = me_data,
#                 family = bernoulli(),
#                 control = list(adapt_delta = 0.95),
#                 sample_prior = F,
#                 save_all_pars = TRUE,
#                 iter = 2000)

#saveRDS(me_model, "../saves/me_effect_model.rds")

me_model <- readRDS("../saves/me_effect_model.rds")

summary(me_model)

fixef <- as_tibble(fixef(me_model), rownames = "term")

ranef <-  ranef(me_model)

age_me_effect <- as_data_frame(ranef$familiar, rownames = "familiar")%>%
  mutate(grand_intercept = fixef%>%filter(term=="Intercept")%>%pull(Estimate),
         grand_slope = fixef%>%filter(term=="z_age")%>%pull(Estimate))%>%
  group_by(familiar) %>%
  tidyr::expand(Estimate.Intercept,Estimate.z_age,grand_intercept,grand_slope,z_age = me_data$age_num - mean(me_data$age_num), mean_age = mean(me_data$age_num))%>%
  mutate(me_effect = plogis(grand_intercept + Estimate.Intercept+(Estimate.z_age+grand_slope)*z_age), 
         age = z_age+mean_age,
         item = familiar)%>%
  select(item, me_effect, age)%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa))


glmm_plot <- ggplot(age_me_effect, aes(x=age,y = me_effect, col = item))+
  geom_hline(yintercept = 0.5, lty=2)+
  geom_line(size = 1)+
  ggtitle("GLMM")+
  labs(x="Age",y="Proportion correct")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d(name = "familiar object")



```


```{r}
ggarrange(data_plot,model_plot,glmm_plot, common.legend = T, legend = "right", nrow = 1, ncol = 3)
```

# MAP based Model predictions for combination

```{r}
# novelty data

novel_data <- read_csv("../data/novelty.csv")%>%
  mutate(age_num = age_num-min(age_num))

# novel_bm <- brm(correct ~ age_num + (1 | subid) + (age_num | agent), 
#           data = novel_data, family = bernoulli(),
#           control = list(adapt_delta = 0.95),
#           sample_prior = F,
#           save_all_pars = F,
#           iter = 10000)
# 
# saveRDS(novel_bm, "../saves/novelty_model.rds")

novel_bm <- readRDS("../saves/novelty_model.rds")

summary(novel_bm)

novel_params <- as_tibble(fixef(novel_bm), rownames = "term")%>%
  select(term,Estimate)%>%
  spread(term, Estimate)

comb_model_data_con <- inv_model_data %>%
  mutate(nov_int = novel_params$Intercept,
         nov_slope = novel_params$age_num,
         prior = plogis(nov_int + age * nov_slope),
         alignment = "congruent")

  
comb_model_data_incon <- inv_model_data %>%
  mutate(nov_int = novel_params$Intercept,
         nov_slope = novel_params$age_num,
         prior = 1- plogis(nov_int + age * nov_slope),
         alignment = "incongruent")


comb_model_data <- bind_rows(
  comb_model_data_con,
  comb_model_data_incon)
```

```{r}
combPred <- '
var allData = dataFromR.data

var output = map(function(row){

    var prior = [row.prior, 1-row.prior]

    var modelPredictions = pragmaticListener({label: "novel_word"}, prior, row.speaker_optimality,row.semantic_knowledge)

    return extend([row.alignment + "/" +  row.item + "/" + row.age, Math.exp(modelPredictions.score(1))])

}, allData)

output


'
```


```{r predictions pragmatic model ex 3 adults}
comb_model_pred<- webppl(
  program_code = paste(rsaUtils, rsaModel, combPred , sep='\n'),
  data =list(data = comb_model_data),
  data_var = "dataFromR"
)


comb_model_predictions <- comb_model_pred %>%
  separate(`0`, into = c("alignment","item", "age"), sep="/")%>%
  mutate(age = as.numeric(age))%>%
  dplyr::rename( me_effect = `1`)%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         age = age + min(me_data$age_num),
         source = "Model")

```


```{r}
p_comb_model <- ggplot(comb_model_predictions, aes(x=age,y = me_effect, col = item))+
  geom_hline(yintercept = 0.5, lty=2)+
  geom_line(size = 1)+
  ggtitle("Combination Model Predictions")+
  labs(x="Age",y="Proportion correct")+
  facet_grid(~alignment)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d()

p_comb_model
```

```{r}
data_comb <- read_csv("../data/combination.csv") %>%
  filter(condition != "train")%>%
  mutate(alignment = condition,
         item = familiarObject)%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(familiarObject), mean_aoa), 
         source = "Data")

p_comb_data <- ggplot(data = data_comb, aes(x = age_num, y = correct, col = item, fill = item)) +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_jitter(width = 0, height = 0.05, alpha = .6)+
  #geom_smooth(method = "glm", method.args = list(family = "binomial"), se = F, alpha = .5)+
  geom_smooth(method = "loess", se = F, alpha = .5, span = 2)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(~alignment)+
  ggtitle("Combination Data")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d()+
  scale_fill_viridis_d()
```

```{r}
ggarrange(p_comb_model,p_comb_data, common.legend = T, legend = "right", nrow = 2, ncol = 1)
```

```{r}
#ggsave("../graphs/combination_model_data.pdf", width = 7, height = 7, scale = 1)
```

```{r}
p_data_model <- ggplot(data = data_comb, aes(x = age_num, y = correct, col = alignment, fill = alignment, lty = source)) +
  geom_hline(yintercept = 0.5, lty=2)+
  #geom_jitter(width = 0, height = 0.05, alpha = .6)+
  #geom_smooth(method = "glm", method.args = list(family = "binomial"), se = F, alpha = .5)+
  geom_smooth(method = "loess", se = F, alpha = .2, span = 2)+
  geom_line(data = comb_model_predictions, aes(x=age,y = me_effect, col = alignment, lty = source), size = 1)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  #facet_grid(condition~familiarObject)+
  facet_grid(~item)+
  #ggtitle("Combination Data")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)

p_data_model
```


```{r}
ggsave("../graphs/combination_model_data_by_item.pdf", width = 8, height = 3, scale = 1.5)
```

## Correlate model predictions and data 

```{r}
binnned_data <- data_comb%>%
  group_by(subage, alignment, item)%>%
  summarize(k = sum(correct), n = n())%>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         data_mean = (a-1)/(a+b-2),
         data_ci_lower  = qbeta(.025, a, b),
         data_ci_upper = qbeta(.975, a, b),
         subage = factor(subage))%>%
  select(-a,-b,-n,-k)


binned_model <- comb_model_predictions %>%
  mutate(subage = cut(age, 
                      breaks = c(2,3,4,5),
                      labels = c(2,3,4)))%>%
  group_by(subage, alignment, item)%>%
  summarise(model_mean = estimate_mode(me_effect),
            model_ci_lower = hdi_lower(me_effect),
            model_ci_upper = hdi_upper(me_effect))


cor_plot <- left_join(
  binnned_data,
  binned_model)


ggplot(data = cor_plot,aes(x = model_mean, y = data_mean, col = item)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_errorbar(aes(ymin = data_ci_lower, ymax = data_ci_upper),width = 0,size = .7)+
  geom_errorbarh(aes(xmin = model_ci_lower, xmax = model_ci_upper), height = 0,size = .7)+
  geom_point(size = 2,  stroke = 1)+
  coord_fixed()+
 stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = model_mean, y = data_mean), inherit.aes = F, size = 3)+
  xlim(0,1)+ylim(0,1)+
  xlab("Model")+
  ylab("Data")+
  facet_grid(~subage)+
  theme_few() + 
  scale_colour_viridis_d(name ="Item")
```

```{r}
#ggsave("../graphs/combination_correlation.pdf", width = 8, height = 4, scale = 1.2)
```


```{r}
ggarrange(
plot_item_params,
plot_global_params,
p_data_model, plot_global_sigma,
ncol = 2, nrow = 2, common.legend = F, widths = c(3,1))

ggsave("../graphs/overview_variant_3_100k.png", width = 12, height = 4, scale = 1.2)

```

## Plotting model predictions

```{r}
model_pred<- sem_know_model %>%
  separate(Parameter, into = c("parameter", "item","age","alignment"), sep = ",")%>%
  filter(parameter == "prediction")%>%
  separate(value, into = c("na", "me_effect"),sep = ",")%>%
  select(-na)%>%
  mutate(age = as.numeric(age),
         me_effect =as.numeric(str_remove(me_effect,"[)]")))


comb_model_pred_agg <- model_pred%>%
  group_by(Chain, alignment,item,age)%>%
  summarise(mean = estimate_mode(me_effect),
            uci = hdi_upper(me_effect),
            lci = hdi_lower(me_effect))%>%
  left_join(aoa_ratings) %>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         age = age + min(me_data$age_num),
         Chain = factor(Chain))

ggplot(comb_model_pred_agg, aes(x=age,y = mean, col = item, fill = item))+
  geom_ribbon(aes(ymin = lci, ymax = uci), alpha = .5, colour = NA)+
  geom_smooth(data = data_comb, aes(x = age_num, y = correct, col), col = "black", fill = "grey",  method = "loess", se = F, alpha = .2, span = 2)+
  geom_hline(yintercept = 0.5, lty=2)+
  geom_line(size = 1)+
  ggtitle("Combination Model Predictions")+
  labs(x="Age",y="Proportion correct")+
  facet_grid(Chain ~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d()+
  scale_fill_viridis_d()

```

```{r}
ggsave("../graphs/model_pred_data.pdf", width = 20, height = 8, scale = 1.2)
```

# MAP based predictions with noise parameter

## Preparing data

```{r}
noise_model_data <- item_params_summary%>%
  select(-uci,-lci)%>%
  spread(parameter, mode) %>%
  left_join(read_csv("../data/combination.csv") %>%
  filter(condition != "train")%>%
  mutate(alignment = condition,
         item = familiarObject,
         age = age_num - (min(age_num))))%>%
  mutate(so_int = global_params_summary%>%filter(token == "speaker_optimality", parameter == "int")%>%pull(mode),
         so_slope = global_params_summary%>%filter(token == "speaker_optimality", parameter == "slope")%>%pull(mode),
    glob_sem_know_int = global_params_summary%>%filter(token == "global_sem", parameter == "int")%>%pull(mode),
         glob_sem_know_slope = global_params_summary%>%filter(token == "global_sem", parameter == "slope")%>%pull(mode),
         speaker_optimality = so_int + so_slope * age,
         semantic_knowledge = plogis(int + slope * age),
         global_semantic_knowledge = plogis(glob_sem_know_int + glob_sem_know_slope * age),
         nov_int = novel_params$Intercept,
         nov_slope = novel_params$age_num,
         prior = ifelse(alignment == "congruent", plogis(nov_int + nov_slope* age),1-plogis(nov_int + age * nov_slope)))%>%
  select(item, alignment,age, speaker_optimality, semantic_knowledge,global_semantic_knowledge, prior, correct)
  
```

## Pragmatics model

```{r}
pragNoise <- '
var allData = dataFromR.data

var model = function(){

  var noise = uniform({a: 0, b: 1}) 

  var output = map(function(row){

    var prior = [row.prior, 1-row.prior]

    var modelPredictions = pragmaticListener({label: "novel_word"}, prior, row.speaker_optimality,row.semantic_knowledge)
    
    var noisyModelPredictions = addNoise(modelPredictions, noise)

    observe(noisyModelPredictions, row.correct)
  
    return extend([row.alignment + "/" +  row.item + "/" + row.age, Math.exp(noisyModelPredictions.score(1))])

}, allData)

 return extend(_.fromPairs(output), {noise: noise})

}
'
```


```{r predictions pragmatic model ex 3 adults}
prag_model_noise<- webppl(
  program_code = paste(rsaUtils, rsaModel, pragNoise , sep='\n'),
  data =list(data = noise_model_data),
  data_var = "dataFromR",
  chains = 3,
  cores = 3,
  inference_opts = list(method = "MCMC", samples = 2000, burn = 1000, verbose = T)
)

saveRDS(prag_model_noise, "../saves/prag_model_noise.rds")

prag_model_noise <- readRDS("../saves/prag_model_noise.rds")


# noise parameter
prag_model_noise_noise_param_summary <- prag_model_noise %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))


# model predictions
prag_model_noise_predictions <- prag_model_noise %>%
  filter(!(Parameter %in% c("noise"))) %>%
  separate(Parameter, into = c("alignment","item", "age"), sep="/")%>%
  mutate(model = "pragmatic_noise")%>%
  group_by(model,age,item, alignment) %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value), 
            ci_upper =hdi_upper(value))%>%
  ungroup()%>%
  mutate(age = as.numeric(age))%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         source = "Model")

```

### Plot model predictions

```{r}
ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_smooth(data = data_comb, aes(x = age_num, y = correct, col = alignment, fill = alignment, lty = source), method = "loess", se = F, alpha = .2, span = 2)+
  geom_line(data = prag_model_noise_predictions, aes(x=age+2,y = mean, col = alignment, lty = source), size = 1)+
  geom_ribbon(data = prag_model_noise_predictions, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = alignment), alpha = .4) +
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(alignment~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)

ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
geom_smooth(data = data_comb_plot, 
            aes(x = age_num, y = correct), 
            col = "black", method = "glm", method.args = list(family = "binomial"), se = T, alpha = .5)+
  geom_pointrange(data = p1, aes(x = subage+.5, y = mean, ymin = ci_lower, ymax = ci_upper))+
  geom_ribbon(data = prag_model_noise_predictions, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = model), alpha = .6) +
  geom_line(data = prag_model_noise_predictions, aes(x=age+2,y = mean, col = model), size = 1, alpha = 1)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_wrap(alignment~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F, fill = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)

p1 <- data_comb%>%
  mutate(alignment = condition)%>%
  group_by(alignment, item ,subage)%>%
  multi_boot_standard(col = "correct")
  

ggplot(p1, aes(x = subage, y = mean))+
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper))+
  facet_grid(condition ~familiarObject)+
  geom_line(aes(group=condition))+
  geom_line(data = prag_model_noise_predictions, aes(x=age+2,y = mean, col = model), size = 1, alpha = 1)+
  geom_ribbon(data = prag_model_noise_predictions, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = model), alpha = .5) +
  theme_few() +


```

## Pragmatics model global semantic knnowledge

```{r}
globalPragNoise <- '
var allData = dataFromR.data

var model = function(){

  var noise = uniform({a: 0, b: 1}) 

  var output = map(function(row){

    var prior = [row.prior, 1-row.prior]

    var modelPredictions = pragmaticListener({label: "novel_word"}, prior, row.speaker_optimality,row.global_semantic_knowledge)
    
    var noisyModelPredictions = addNoise(modelPredictions, noise)

    observe(noisyModelPredictions, row.correct)
  
    return extend([row.alignment + "/" +  row.item + "/" + row.age, Math.exp(noisyModelPredictions.score(1))])

}, allData)

 return extend(_.fromPairs(output), {noise: noise})

}
'
```


```{r predictions pragmatic model ex 3 adults}
global_prag_model_noise<- webppl(
  program_code = paste(rsaUtils, rsaModel, globalPragNoise , sep='\n'),
  data =list(data = noise_model_data),
  data_var = "dataFromR",
  chains = 3,
  cores = 3,
  inference_opts = list(method = "MCMC", samples = 2000, burn = 1000, verbose = T)
)

saveRDS(global_prag_model_noise, "../saves/global_prag_model_noise.rds")

global_prag_model_noise <- readRDS("../saves/global_prag_model_noise.rds")


# noise parameter
global_prag_model_noise_noise_param_summary <- global_prag_model_noise %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))


# model predictions
global_prag_model_noise_predictions <- global_prag_model_noise %>%
  filter(!(Parameter %in% c("noise"))) %>%
  separate(Parameter, into = c("alignment","item", "age"), sep="/")%>%
  mutate(model = "global_pragmatic_noise")%>%
  group_by(model,age,item, alignment) %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value), 
            ci_upper =hdi_upper(value))%>%
  ungroup()%>%
  mutate(age = as.numeric(age))%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         source = "Model")

```

### Plot model predictions

```{r}
ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_smooth(data = data_comb, aes(x = age_num, y = correct, col = alignment, fill = alignment, lty = source), method = "loess", se = F, alpha = .2, span = 2)+
  geom_line(data = global_prag_model_noise_predictions, aes(x=age+2,y = mean, col = alignment, lty = source), size = 1)+
  geom_ribbon(data = global_prag_model_noise_predictions, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = alignment), alpha = .4) +
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_wrap(~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)

```

## Flat prior model

```{r}
flatNoise <- '
var allData = dataFromR.data

var model = function(){

  var noise = uniform({a: 0, b: 1}) 

  var output = map(function(row){

    var prior = [row.prior, 1-row.prior]

    var modelPredictions = pragmaticListener({label: "novel_word"}, [.5,.5], row.speaker_optimality,row.semantic_knowledge)
    
    var noisyModelPredictions = addNoise(modelPredictions, noise)

    observe(noisyModelPredictions, row.correct)
  
    return extend([row.alignment + "/" +  row.item + "/" + row.age, Math.exp(noisyModelPredictions.score(1))])

}, allData)

 return extend(_.fromPairs(output), {noise: noise})

}
'
```


```{r predictions pragmatic model ex 3 adults}
flat_model_noise<- webppl(
  program_code = paste(rsaUtils, rsaModel, flatNoise , sep='\n'),
  data =list(data = noise_model_data),
  data_var = "dataFromR",
  chains = 3,
  cores = 3,
  inference_opts = list(method = "MCMC", samples = 2000, burn = 1000, verbose = T)
)


saveRDS(flat_model_noise, "../saves/flat_model_noise.rds")

flat_model_noise <- readRDS("../saves/flat_model_noise.rds")


# noise parameter
flat_model_noise_noise_param_summary <- flat_model_noise %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))


# model predictions
flat_model_noise_predictions <- flat_model_noise %>%
  filter(!(Parameter %in% c("noise"))) %>%
  separate(Parameter, into = c("alignment","item", "age"), sep="/")%>%
  mutate(model = "flat_prior_noise")%>%
  group_by(model,age,item, alignment) %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value), 
            ci_upper =hdi_upper(value))%>%
  ungroup()%>%
  mutate(age = as.numeric(age))%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         source = "Model")

```

### Plot model predictions

```{r}
ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_smooth(data = data_comb, aes(x = age_num, y = correct, col = alignment, fill = alignment, lty = source), method = "loess", se = F, alpha = .2, span = 2)+
  geom_line(data = flat_model_noise_predictions, aes(x=age+2,y = mean, col = alignment, lty = source), size = 1)+
  geom_ribbon(data = flat_model_noise_predictions, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = alignment), alpha = .4) +
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_wrap(~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)
```

## Prior only model

```{r}
priorOnlyNoise <- '
var allData = dataFromR.data

var model = function(){

  var noise = uniform({a: 0, b: 1}) 

  var output = map(function(row){

    var prior = [row.prior, 1-row.prior]

      var modelPredictions = Infer({method: "enumerate", model: function(){
      var obj = sample( Categorical({vs: all_objects, ps: prior}));
      return obj.shape == "novel_object" ? 1 : 0
       }})
    
    var noisyModelPredictions = addNoise(modelPredictions, noise)

    observe(noisyModelPredictions, row.correct)
  
    return extend([row.alignment + "/" +  row.item + "/" + row.age, Math.exp(noisyModelPredictions.score(1))])

}, allData)

 return extend(_.fromPairs(output), {noise: noise})

}
'
```


```{r predictions pragmatic model ex 3 adults}
prior_only_model_noise<- webppl(
  program_code = paste(rsaUtils, rsaModel, priorOnlyNoise , sep='\n'),
  data =list(data = noise_model_data),
  data_var = "dataFromR",
  chains = 3,
  cores = 3,
  inference_opts = list(method = "MCMC", samples = 2000, burn = 1000, verbose = T)
)


saveRDS(prior_only_model_noise, "../saves/prior_only_model_noise.rds")

prior_only_model_noise <- readRDS("../saves/prior_only_model_noise.rds")

# noise parameter
prior_only_model_noise_noise_param_summary <- prior_only_model_noise %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))


# model predictions
prior_only_model_noise_predictions <- prior_only_model_noise %>%
  filter(!(Parameter %in% c("noise"))) %>%
  separate(Parameter, into = c("alignment","item", "age"), sep="/")%>%
  mutate(model = "prior_only_noise")%>%
  group_by(model,age,item, alignment) %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value), 
            ci_upper =hdi_upper(value))%>%
  ungroup()%>%
  mutate(age = as.numeric(age))%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         source = "Model")

```

### Plot model predictions

```{r}
ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_smooth(data = data_comb, aes(x = age_num, y = correct, col = alignment, fill = alignment, lty = source), method = "loess", se = F, alpha = .2, span = 2)+
  geom_line(data = prior_only_model_noise_predictions, aes(x=age+2,y = mean, col = alignment, lty = source), size = 1)+
  geom_ribbon(data = prior_only_model_noise_predictions, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = alignment), alpha = .4) +
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_wrap(~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)
```

## Compare noise parameters

```{r}
noise_plot <- bind_rows(
  prag_model_noise %>%
    filter(Parameter %in% c("noise"))%>%
    mutate(Model = "Pragmatic"),
  global_prag_model_noise %>%
    filter(Parameter %in% c("noise"))%>%
    mutate(Model = "Pragmatic global"),
  flat_model_noise %>%
    filter(Parameter %in% c("noise"))%>%
    mutate(Model = "Flat Prior"),
  prior_only_model_noise %>%
    filter(Parameter %in% c("noise"))%>%
    mutate(Model = "Prior Only")
)

ggplot(noise_plot, aes(x = value, col = Model, fill = Model))+
  geom_density(alpha = .7, bw = .01)+
  theme_few()+
  xlim(0,1)+
  scale_color_ptol()+
  scale_fill_ptol()+
  theme()
  

```

```{r}
ggsave("../graphs/noise.pdf", width = 6, height = 3, scale = 1.2)
```

## Compare predictions

```{r}

noise_model_pred <- bind_rows(
  prag_model_noise_predictions,
  global_prag_model_noise_predictions,
  prior_only_model_noise_predictions,
  flat_model_noise_predictions
)

data_comb_plot<-data_comb%>%
  mutate(model = "data")

ggplot() +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_smooth(data = data_comb_plot, aes(x = age_num, y = correct), col = "black", method = "loess", se = T, alpha = .5, span = 2)+
  geom_line(data = noise_model_pred, aes(x=age+2,y = mean, col = model), size = 1)+
  geom_ribbon(data = noise_model_pred, aes(x = age+2, ymin = ci_lower, ymax = ci_upper, fill = model), alpha = .4) +
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(alignment~item)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F, fill = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)
```

## Correlate predictions with data 

```{r}
binned_noise_model <- prag_model_noise %>%
  filter(!(Parameter %in% c("noise"))) %>%
  separate(Parameter, into = c("alignment","item", "age"), sep="/")%>%
  mutate(age = as.numeric(age) + min(me_data$age_num),
         subage = factor(cut(age, 
                      breaks = c(2,3,4,5),
                      labels = c(2,3,4))))%>%
  group_by(subage, alignment, item)%>%
  summarise(model_mean = estimate_mode(value),
            model_ci_lower = hdi_lower(value),
            model_ci_upper = hdi_upper(value))



cor_plot_2 <- bind_rows(
left_join(binned_model, binnned_data)%>%mutate(model = "Parameter free"), 
left_join(binned_noise_model,binnned_data)%>%mutate(model = "Noise")
)

ggplot(data = cor_plot_2,aes(x = model_mean, y = data_mean, col = item)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_errorbar(aes(ymin = data_ci_lower, ymax = data_ci_upper),width = 0,size = .7)+
  geom_errorbarh(aes(xmin = model_ci_lower, xmax = model_ci_upper), height = 0,size = .7)+
  geom_point(size = 2,  stroke = 1)+
  coord_fixed()+
 stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = model_mean, y = data_mean), inherit.aes = F, size = 3)+
  xlim(0,1)+ylim(0,1)+
  xlab("Model")+
  ylab("Data")+
  facet_grid(~subage)+
  theme_few() + 
  scale_colour_viridis_d(name ="Item")
```

```{r}
ggsave("../graphs/cor_plot_noise.pdf", width = 10, height = 4, scale = .8)
```


# OLD: Comparing parameters between model runs

## Base model

```{r}
baseMeModel <- '

var sem_knowledge = 1

var priorProbs = [.5,.5]

var speakerOptimality = 1

var modelPredictions = pragmaticListener({label: "novel_word"}, priorProbs, speakerOptimality, sem_knowledge)

  modelPredictions

'
```

```{r}
webppl(program_code = paste(rsaUtils, rsaModel, baseMeModel,sep='\n'))
```

## Prior model 

```{r}
prior_params <- '
  
var priorData = dataFromR.data

var priorSubjects = levels(priorData, "subid")

display(priorData)

var model  = function(){

  var prior_slope = uniformDrift({a: -2, b: 2, width: 0.4})
  var prior_int = uniformDrift({a: -2, b: 2, width: 0.4})

  var prior_subject_sigma = uniformDrift({a: 0, b:3, width: 0.2}) 

  var priorSampleSub = function(age){
    return gaussianDrift({ mu: age, sigma: prior_subject_sigma, width: 0.1 })
  }

  foreach(function(subid){
      var priorSubjectData = _.filter(priorData, {subid: subid})

      //display(subjects)

      var subj_age = priorSubjectData[0].age

      var subjectPrior = priorSampleSub(subj_age)

    foreach(function(row){
  
      var priorReg = logistic(prior_int + prior_slope * subjectPrior)

      var prior = [priorReg, 1-priorReg]

      var modelPredictions = Infer({method: "enumerate", model: function(){
      var obj = sample( Categorical({vs: all_objects, ps: prior}));
      return obj.shape == "novel_object" ? 1 : 0
       }})
    
     observe(modelPredictions, row.correct)

    }, priorSubjectData)

  }, priorSubjects)

  query.add(["intercept"], [prior_int])
  query.add(["slope"], [prior_slope])
  query.add(["sigma"], [prior_subject_sigma])
 
  return query 

}

'
```

```{r}

prior_model<- webppl(
  program_code = paste(rsaUtils, rsaModel, prior_params , sep='\n'),
  data = list(data = prior_model_data),
  data_var = "dataFromR",
  model_var = "model",
  chains = 1,
  cores = 1,
  inference_opts = list(method = "MCMC", samples = 1, burn = 1, verbose = T)
)

saveRDS(prior_model, "../saves/prior_model.rds")

prior_model <- readRDS("../saves/prior_model.rds")

prior <- prior_model%>%
  mutate(value = unlist(value))

prior_summary <- prior%>%
  group_by(Parameter)%>%
  summarise(param_map = estimate_mode(value),
            param_uci = hdi_upper(value),
            param_lci = hdi_lower(value))


```

#### Visualizing parameters

```{r}
ggplot(prior, aes(x = value, fill = factor(Chain)))+
  geom_density(alpha = 0.3)+
  xlab("Parameter")+
  facet_grid(Parameter~., scales = 'free')+
  theme_few()

```

#### Comparison to brm models

```{r}

# same model as webppl model  implemented in brms

novel_bm <- brm(correct ~ age + (1 | subid), 
          data = prior_model_data, family = bernoulli(),
          control = list(adapt_delta = 0.95),
          sample_prior = F,
          save_all_pars = TRUE,
          iter = 20000)

summary(novel_bm)

saveRDS(novel_bm, "../saves/novelty_model_brm.rds")
```


```{r}
# brm model from last chunk
novel_bm <- readRDS("../saves/novelty_model_brm.rds")

#summary(novel_bm)

brm_params <- as_tibble(fixef(novel_bm), rownames = "term")%>%
  select(term,Estimate)%>%
  spread(term, Estimate)

brm_plot <- tibble(
  age = prior_model_data$age,
  y = plogis(brm_params$Intercept + brm_params$age * age),
  model = "brm"
)


#brm model with additional random intercept and slope for item
novel_bm_full <- readRDS("../saves/novelty_model.rds")

#summary(novel_bm_full)

brm_full_params <- as_tibble(fixef(novel_bm_full), rownames = "term")%>%
  select(term,Estimate)%>%
  spread(term, Estimate)

brm_full_plot <- tibble(
  age = prior_model_data$age,
  y = plogis(brm_full_params$Intercept + brm_full_params$age_num * age),
  model = "brm_full"
)


# webppl model
webppl_plot <- tibble(
  age = prior_model_data$age,
  y = plogis(prior_summary%>%filter(Parameter == "intercept")%>%pull(param_map) + prior_summary%>%filter(Parameter == "slope")%>%pull(param_map) * age),
  model = "webppl"
)

prior_comp_plot <- bind_rows(
  brm_plot,
  webppl_plot,
  brm_full_plot
  
)
```


```{r}
ggplot(prior_comp_plot, aes(x = age, y = y, col = model))+
  geom_line()+
  ylim(0,1)+
  theme_few()+
  scale_color_ptol()
```

## sem knowledge

```{r}

sem_know_run_summary <- item_params%>%
  spread(parameter,value)%>%
  group_by(Chain,item)%>%
  summarise(overall_int = estimate_mode(int),
            overall_slope = estimate_mode(slope))


sem_know_run_iteration <- item_params%>%
  spread(parameter,value)

sem_know_chain <- sem_know_run_iteration %>%
  left_join(sem_know_run_summary)

chain_comp <- sem_know_chain %>%
  left_join(expand.grid(
      age = unique(me_model_data$age),
      item = unique(me_model_data$familiar)), .) %>%
  mutate(sem_know_ci = plogis(int + slope * age),
         sem_know = plogis(overall_int + overall_slope * age)) %>%
  group_by(Chain,item, age) %>%
  summarise(sem_know = estimate_mode(sem_know),
            sem_know_uci = hdi_upper(sem_know_ci),
            sem_know_lci = hdi_lower(sem_know_ci))

ggplot(chain_comp, aes(x = age+2, y= sem_know, fill = factor(Chain)))+
  geom_line(size = .3, aes(col = factor(Chain)))+
  geom_ribbon(aes(ymin=sem_know_lci, ymax = sem_know_uci), alpha = .05)+
  ylab("Semantic knowledge")+
  xlab("Age")+
  ylim(0,1)+
  facet_wrap(~item, nrow = 3)+
  theme_few()+
  scale_colour_discrete()+
  scale_fill_discrete()

```

## speaker optimality

```{r}

so_run_iteration <- global_params%>%
  spread(parameter, value)

so_run_summary <- so_run_iteration%>%
  group_by(Chain)%>%
  summarise(overall_int = estimate_mode(int),
            overall_slope = estimate_mode(slope))

so_run <- so_run_iteration %>%
  left_join(so_run_summary)

so_run_comp <-so_run%>%
  group_by(Chain,Iteration)%>%
  tidyr::expand(int,slope,overall_int,overall_slope,age = unique(me_model_data$age))%>%
  mutate(so_ci = int + slope * age,
         so = overall_int + overall_slope * age) %>%
  group_by(Chain, age) %>%
  summarise(so = estimate_mode(so),
            so_uci = hdi_upper(so_ci),
            so_lci = hdi_lower(so_ci))

ggplot(so_run_comp, aes(x = age+2, y= so, fill = factor(Chain)))+
  geom_line(size = .4, aes(col = factor(Chain)))+
  geom_ribbon(aes(ymin=so_lci, ymax = so_uci), alpha = .05)+
  ylab("speaker optimality")+
  xlab("Age")+
  #facet_grid(~run)+
  theme_few()+
  scale_colour_discrete()+
  scale_fill_discrete()

```


## Correlations between parameters for semantic knowledge

```{r}
ggplot(item_params%>%filter(parameter == "int"), aes(x = value))+
  geom_density(alpha = 0.3)+
  xlab("Parameter")+
  facet_grid(Chain~item, scales = 'free')+
  theme_few()

#ggsave("../graphs/item_by_chain.pdf", width = 15, height = 15, scale = 1.5)
```

```{r}
item_params_cor <- item_params %>%
  group_by(Chain,item, parameter)%>%
  summarise(mode = estimate_mode(value))
```

```{r}
library(corrr)
library(corrplot)

## correaltion at 2
cor_int_2 <- item_params_cor%>%
  spread(Chain, mode) %>%
  filter(parameter == "int")%>%
  select(-parameter)%>%
  ungroup()%>%
  select(-item)%>%
  correlate(method = "spearman")%>%
  gather(key, value)%>%
  filter(key !="rowname")%>%
  na.omit()%>%
  mutate(value = as.numeric(value),
         age = "2")

cor_int_3 <- item_params_cor%>%
  spread(parameter, mode)%>%
  mutate(mode = int + slope)%>%
  select(Chain, item, mode)%>%
  spread(Chain, mode)%>%
  ungroup()%>%
  select(-item)%>%
  correlate(method = "spearman")%>%
  gather(key, value)%>%
  filter(key !="rowname")%>%
  na.omit()%>%
  mutate(value = as.numeric(value),
         age = "3")

cor_int_4 <- item_params_cor%>%
  spread(parameter, mode)%>%
  mutate(mode = int + 2*slope)%>%
  select(Chain, item, mode)%>%
  spread(Chain, mode)%>%
  ungroup()%>%
  select(-item)%>%
  correlate(method = "spearman")%>%
  gather(key, value)%>%
  filter(key !="rowname")%>%
  na.omit()%>%
  mutate(value = as.numeric(value),
         age = "4")


cor_int_5 <- item_params_cor%>%
  spread(parameter, mode)%>%
  mutate(mode = int + 3*slope)%>%
  select(Chain, item, mode)%>%
  spread(Chain, mode)%>%
  ungroup()%>%
  select(-item)%>%
  correlate(method = "spearman")%>%
  gather(key, value)%>%
  filter(key !="rowname")%>%
  na.omit()%>%
  mutate(value = as.numeric(value),
         age = "5")


cor_hist <- 
bind_rows(
  cor_int_2,
  cor_int_3,
  cor_int_4,
  cor_int_5
)


ggplot(cor_hist, aes(x = value, fill =age))+
  geom_density(alpha = .5)+
  xlim(-1,1)+
  xlab("Correlation")+
  theme_few()


#ggsave("../graphs/corr_int_by_age.pdf", width = 5, height = 4, scale = 1.5)

```

# Model (No nesting within participants)
```{r}
semStr <- '
var data = dataFromR.data

var priorProbs = [.5,.5]

var familiars = levels(data, "familiar")

var model  = function(){

  var speakerOptimalityParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 4, width: 0.5})
  }

  var globalLineParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 2, width: 0.5})
  }

  var itemVariability = {
    intercept: uniformDrift({a: 0, b: 2, width: 0.2}),
    slope: uniformDrift({a: 0, b: 1, width: 0.2})
  }

  // conceptually it might not make sense to have two sigmas, but maybe it does
  // intercept sigma probably will be smaller than slope sigma
  // var item_int_sigma = uniformDrift({a: 0, b: 3, width: 0.2})
  
  foreach(function(cndtn){

    var conditionData = _.filter(data, {familiar: cndtn})
    
      var itemLineParameters = {
        intercept: gaussianDrift({
          mu: globalLineParameters.intercept, 
          sigma: itemVariability.intercept, 
          width: 0.5
        }),
     //   slope: gaussianDrift({
     //     mu: globalLineParameters.slope, 
      //    sigma: itemVariability.slope, 
      //    width: 0.5
     //   })
      slope: Math.pow(gaussianDrift({
         mu: globalLineParameters.slope, 
        sigma: itemVariability.slope, 
         width: 0.5
      }), 2)
      }    

    foreach(function(row){

      var age = row.age

      //display(age)

      var sem_knowledge = logistic(itemLineParameters.intercept + 
          itemLineParameters.slope * age)

      var speakerOptimality = speakerOptimalityParameters.intercept  + speakerOptimalityParameters.slope * age

      var modelPredictions = pragmaticListener({label: "novel_word"}, 
            priorProbs, speakerOptimality, sem_knowledge)
      // display(modelPredictions.score(row.correct))
      
      observe(modelPredictions, row.correct)
      
      // query.add(["modelPrediction", cndtn, age], [age, Math.exp(modelPredictions.score(1))])

    }, conditionData)
    
    query.add(["parameter","items", cndtn], [itemLineParameters.intercept, itemLineParameters.slope])

  }, familiars)

  query.add(["parameter","parameters","speaker_optimality"], [speakerOptimalityParameters.intercept, speakerOptimalityParameters.slope])
  query.add(["parameter","parameters", "global_sem"], [globalLineParameters.intercept, globalLineParameters.slope])
  query.add(["parameter","sigma", "global_sem_sigmas"], [itemVariability.intercept, itemVariability.slope])
  
  return query 
}
'
```

```{r}
me_model_data <- read_csv("../data/me.csv")%>%
    filter(trial != "train1",
           trial != "train2")%>%
  mutate(age = age_num - min(age_num))

sem_know_model<- webppl(
  program_code = paste(rsaUtils, rsaModel, semStr , sep='\n'),
  data = list(data = me_model_data),
  data_var = "dataFromR",
  model_var = "model",
  chains = 3,
  cores = 3,
  inference_opts = list(method = "incrementalMH",
                        samples = 100000,
                        burn = 50000,
                        verbose = T,
                        lag = 1,
                        verboseLag = 5000)
)



#saveRDS(sem_know_model, "../saves/sem_know_model_variant2_100k.rds")
```

