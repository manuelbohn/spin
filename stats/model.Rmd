---
title: "Model"
output: html_document
---

## Model utilities

```{r}
library(tidyverse)
library(rwebppl)
library(brms)
library(coda)
library(ggthemes)
library(readxl)
library(ggpubr)
library(stringr)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}


hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}

me_data <- read_csv("../data/me.csv")

```

```{r rsaUtils}
rsaUtils <- '
var all_objects = [
{ shape: "novel_object"},  
{ shape: "familiar_object"}
]

var labels = ["novel_word","familiar_word"]


var lexicon1 = function(utterance, obj, sem_knowledge){
  utterance.label == "novel_word" ? obj.shape == "novel_object" :
  utterance.label == "familiar_word" ? flip(sem_knowledge) ? 
    obj.shape == "familiar_object" :  
  flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" : 
  true
}

var lexicon2 = function(utterance, obj, sem_knowledge){
  utterance.label == "novel_word" ? obj.shape == "familiar_object" :
  utterance.label == "familiar_word" ? flip(sem_knowledge) ? 
    obj.shape == "familiar_object" : 
  flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" : 
  true
}

var lexiconObjects = {
"novel_word = novel_object": {
novel_object: "novel_word", familiar_object: "familiar_word"
},
"novel_word = familiar_object": {
novel_object: "familiar_word", familiar_object: "familiar_word"
},
}

var lexiconObject = {
"novel_word = novel_object": lexicon1,
"novel_word = familiar_object" : lexicon2
}

var utterancePrior = function(){ return uniformDraw([ {label: "novel_word"}, {label: "familiar_word"}]) }

var LexiconPrior = Categorical({vs: ["novel_word = novel_object","novel_word = familiar_object" ], ps: [1, 1]})

var foreach = function(fn, lst) {
    var foreach_ = function(i) {
        if (i < lst.length) {
            fn(lst[i]);
            foreach_(i + 1);
        }
    };
    foreach_(0);
};

var logistic = function(x) {1 / (1 + Math.exp(-x))}

var levels = function(df, label){
  return _.uniq(_.map(df, label));
}
'
```

```{r rsa model}
rsaModel <- '
var literalListener = cache(function(utterance, priorProbs, sem_knowledge){
  Infer({method: "enumerate", model: function(){
    var lexiconName = sample(LexiconPrior); 
    var lexicon = lexiconObject[lexiconName];
    var obj = sample( Categorical({vs: all_objects, ps: [.5,.5]}));
    if ("label" in utterance) {
      var truthValue = lexicon(utterance, obj, sem_knowledge);
      condition(truthValue)
    }
    return obj.shape 
  }})
}, 10000)

var speaker = cache(function(obj, lexiconName, priorProbs, speakerOptimality, sem_knowledge){
  Infer({method: "enumerate", model: function(){
    var utterance = utterancePrior();
    var L0 = literalListener(utterance, priorProbs, sem_knowledge);
    factor(speakerOptimality * L0.score(obj.shape))
    return utterance
  }})
}, 10000)

var pragmaticListener = cache(function(utterance, priorProbs, speakerOptimality, sem_knowledge){
  Infer({method: "enumerate", model: function(){
    var lexiconName = sample(LexiconPrior);
    var obj = sample( Categorical({vs: all_objects, ps: priorProbs}));
    var S1 = speaker(obj, lexiconName, priorProbs, speakerOptimality, sem_knowledge);
    observe(S1, utterance)
    return obj.shape == "novel_object" ? 1 : 0
  }})
}, 10000)

'
```


```{r}
baseMeModel <- '

var sem_knowledge = 1

var priorProbs = [.5,.5]

var speakerOptimality = 1

var modelPredictions = pragmaticListener({label: "novel_word"}, priorProbs, speakerOptimality, sem_knowledge)

modelPredictions

'
```


```{r}
webppl(program_code = paste(rsaUtils, rsaModel, baseMeModel,sep='\n'))
```

# Inferring semantic knowledge
```{r}
semStr <- '
var data = dataFromR.data

var priorProbs = [.5,.5]

var familiars = levels(data, "familiar")

var model  = function(){

  var speakerOptimalityParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 4, width: 0.5})
  }

  var globalLineParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 2, width: 0.5})
  }

  var itemVariability = {
    intercept: uniformDrift({a: 0, b: 2, width: 0.2}),
    slope: uniformDrift({a: 0, b: 1, width: 0.2})
  }

  // conceptually it might not make sense to have two sigmas, but maybe it does
  // intercept sigma probably will be smaller than slope sigma
  // var item_int_sigma = uniformDrift({a: 0, b: 3, width: 0.2})
  
  foreach(function(cndtn){

    var conditionData = _.filter(data, {familiar: cndtn})
    
      var itemLineParameters = {
        intercept: gaussianDrift({
          mu: globalLineParameters.intercept, 
          sigma: itemVariability.intercept, 
          width: 0.5
        }),
        slope: gaussianDrift({
          mu: globalLineParameters.slope, 
          sigma: itemVariability.slope, 
          width: 0.5
        })
      }    

    foreach(function(row){

      var age = row.age

      //display(age)

      var sem_knowledge = logistic(itemLineParameters.intercept + 
          itemLineParameters.slope * age)

      var speakerOptimality = speakerOptimalityParameters.intercept  + speakerOptimalityParameters.slope * age

      var modelPredictions = pragmaticListener({label: "novel_word"}, 
            priorProbs, speakerOptimality, sem_knowledge)
      // display(modelPredictions.score(row.correct))
      
      observe(modelPredictions, row.correct)
      
      // query.add(["modelPrediction", cndtn, age], [age, Math.exp(modelPredictions.score(1))])

    }, conditionData)
    
    query.add(["parameter","items", cndtn], [itemLineParameters.intercept, itemLineParameters.slope])

  }, familiars)

  query.add(["parameter","parameters","speaker_optimality"], [speakerOptimalityParameters.intercept, speakerOptimalityParameters.slope])
  query.add(["parameter","parameters", "global_sem"], [globalLineParameters.intercept, globalLineParameters.slope])
  query.add(["parameter","sigma", "global_sem_sigmas"], [itemVariability.intercept, itemVariability.slope])
  
  return query 
}
'
```

```{r}
me_model_data <- read_csv("../data/me.csv")%>%
    filter(trial != "train1",
           trial != "train2")%>%
  mutate(age = age_num - min(age_num))

sem_know_model<- webppl(
  program_code = paste(rsaUtils, rsaModel, semStr , sep='\n'),
  data = list(data = me_model_data),
  data_var = "dataFromR",
  model_var = "model",
  chains = 4,
  cores = 4,
  inference_opts = list(method = "incrementalMH",
                        samples = 4000,
                        burn = 2000,
                        verbose = T,
                        lag = 1,
                        verboseLag = 1000)
)


```

# Model runs

```{r}
# load this for model parameters with so_slope (0:2)
#sem_know_model <- readRDS("../saves/sem_know_new_model.rds")

# load this for model parameters with wider so_slope (0:4) and wider sigma on sem_int
#sem_know_model <- readRDS("../saves/sem_know_model_wide_sig.rds")

# load this for model parameters with wider so_slope (0:4) and wider sigma on sem_int and forced positive slope for semantic knowledge
#sem_know_model <- readRDS("../saves/sem_know_model_pos_slope.rds")

# load this for model parameters with wider so_slope (0:4) and wider sigma on sem_int and with random intercepts for subjects
#sem_know_model <- readRDS("../saves/sem_know_model_wide_sig_ind.rds")
 
# load this for model parameters with wider so_slope (0:4) and wider sigma on sem_int, forced positive slope for semantic knowledge and with random intercepts for subjects
sem_know_model <- readRDS("../saves/sem_know_model_wide_sig_pos_slope_ind.rds")


#saveRDS(sem_know_model, "../saves/sem_know_model_wide_sig_ind.rds")

```

# Summarizing parameters

```{r}

# model_pred <- sem_know_model %>%
#   separate(Parameter, into = c("type", "item","x"))%>%
#   filter(type == "modelPrediction")%>%
#   separate(value, into = c("age", "pred"),sep = ", ")%>%
#   mutate(age = as.numeric(str_remove_all(age, "[(c]")),
#          pred = as.numeric(str_remove(pred, "[)]")))%>%
#   select(-x)


item_params<- sem_know_model %>%
  separate(Parameter, into = c("parameter", "type","item"), sep = ",")%>%
  filter(parameter == "parameter", type == "items")%>%
  separate(value, into = c("int", "slope"),sep = ", ")%>%
  mutate(int = as.numeric(str_remove_all(int, "[(c]")),
         slope = as.numeric(str_remove(slope, "[)]")))%>%
  gather(parameter, value, int, slope)


global_params <- sem_know_model %>%
  separate(Parameter, into = c("parameter", "type","token"), sep = ",")%>%
  filter(parameter == "parameter", type == "parameters")%>%
    separate(value, into = c("int", "slope"),sep = ", ")%>%
    mutate(int = as.numeric(str_remove_all(int, "[(c]")),
         slope = as.numeric(str_remove(slope, "[)]")))%>%
    gather(parameter, value, int, slope)



global_sigma<- sem_know_model %>%
  separate(Parameter, into = c("parameter", "type","token"), sep = ",")%>%
  filter(parameter == "parameter", type == "sigma", token != "global_subject_sigmas")%>%
    separate(value, into = c("sigma_int", "sigma_slope"),sep = ", ")%>%
    mutate(sigma_int = as.numeric(str_remove_all(sigma_int, "[(c]")),
         sigma_slope = as.numeric(str_remove(sigma_slope, "[)]")))%>%
    gather(parameter, value, sigma_int, sigma_slope)


subj_sigma<- sem_know_model %>%
  separate(Parameter, into = c("parameter", "type","token"), sep = ",")%>%
  filter(parameter == "parameter", token == "global_subject_sigmas")%>%
  separate(value, into = c("na", "sigma"),sep = ", ")%>%
  select(-na)%>%
  mutate(sigma = as.numeric(str_remove(sigma, "[)]")))


subject_params<- sem_know_model %>%
  separate(Parameter, into = c("parameter", "type","id"), sep = ",")%>%
  filter(parameter == "parameter", type == "subj_intercept")%>%
  separate(value, into = c("na", "int"),sep = ", ")%>%
  select(-na)%>%
  mutate(int = as.numeric(str_remove(int, "[)]")))


model_pred<- sem_know_model %>%
  separate(Parameter, into = c("parameter", "item","age"), sep = ",")%>%
  filter(parameter == "prediction")%>%
  separate(value, into = c("alignment", "me_effect"),sep = ", ")%>%
  mutate(age = as.numeric(age),
         me_effect =str_remove(me_effect,"[)]"),
         me_effect =str_remove(me_effect,'"'),
         me_effect =as.numeric(str_remove(me_effect,'"')),
         alignment = str_remove(alignment,'c[(]'),
         alignment =str_remove(alignment,'"'),
         alignment =str_remove(alignment,'"'))


#### summaries

# model_pred_summary <- model_pred %>%
#   group_by(item,age)%>%
#   summarise(mode = estimate_mode(pred),
#             uci = hdi_upper(pred),
#             lci = hdi_lower(pred))

item_params_summary <- item_params %>%
  group_by(item, parameter)%>%
  summarise(mode = estimate_mode(value),
            uci = hdi_upper(value),
            lci = hdi_lower(value))


global_params_summary <- global_params %>%
  group_by(token, parameter)%>%
  summarise(mode = estimate_mode(value),
            uci = hdi_upper(value),
            lci = hdi_lower(value))
  
```

```{r}
plot_item_params <- ggplot(item_params, aes(x = value, fill = factor(Chain)))+
  geom_density(alpha = 0.3)+
  xlab("Parameter")+
  facet_grid(parameter~item, scales = 'free')+
  theme_few()

plot_item_params
```

```{r}
#ggsave("../graphs/item_params_by_chain.pdf", width = 8, height = 3, scale = 1.5)
```



```{r}

plot_global_params <- ggplot(global_params, aes(x = value, fill = factor(Chain)))+
  geom_density(alpha = 0.3)+
  xlab("Parameter")+
  facet_grid(parameter~token, scales = 'free')+
  theme_few()

plot_global_params
```

```{r}
plot_global_sigma <- ggplot(global_sigma, aes(x = value, fill = factor(Chain)))+
  geom_density(alpha = 0.3)+
  xlab("Sigma")+
  facet_grid(~parameter, scales = 'free')+
  theme_few()

plot_global_sigma
```


```{r}
plotg_subj_sigma <- ggplot(subj_sigma, aes(x = sigma, fill = factor(Chain)))+
  geom_density(alpha = 0.3)+
  xlab("Subject Sigma")+
  theme_few()

plotg_subj_sigma
```


```{r}

select_id <- me_model_data %>%
  filter(age < 1.3,
         age >1)%>%
  group_by(age,subid)%>%
  summarise(mean = mean(correct))%>%
  mutate(id = subid)


subject_params <- subject_params %>%
  left_join(select_id)%>%
  filter(mean != "NA")%>%
  ungroup()


ggplot(subject_params, aes(x = int, fill = factor(Chain)))+
  geom_density(alpha = 0.3)+
  geom_vline(aes(xintercept = age), size = 1.5)+
  #geom_vline(aes(xintercept = mean), size = 1.5, col = "red")+
  facet_grid(id+ round(mean,2)~.)+
  xlab("Subject Intercept")+
  theme_few()



ggsave("../graphs/subj_intercept.pdf", width = 5, height = 8, scale = 1.5)

```


## Plotting semantic knowledge
```{r}
aoa_ratings_all <- read_xlsx(path = "../data/words_aoa_ratings.xlsx", sheet = 1)

aoa_ratings <- aoa_ratings_all %>%
  filter(Word %in% item_params_summary$item )%>%
  mutate(mean_aoa = as.numeric(Rating.Mean),
         item = Word)%>%
  select(item,mean_aoa)

plot_sem_know <- item_params_summary%>%
  select(-uci,-lci)%>%
  spread(parameter, -item) %>%
  left_join(expand.grid(
      age = unique(me_model_data$age),
      item = unique(me_model_data$familiar)), .) %>%
  mutate(sem_know = plogis(int + slope * age)) %>%
  left_join(aoa_ratings) %>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa))

ggplot(plot_sem_know, aes(x = age+2, y= sem_know, col = item, group = item))+
  geom_line(size = 1)+
  ylab("Semantic knowledge")+
  xlab("Age")+
  ylim(0,1)+
  theme_few()+
  scale_colour_viridis_d(name = "familiar object")
```

### Comparing parameters between model runs

#### sem knowledge

```{r}
# me_model_data <- read_csv("../data/me.csv")%>%
#     filter(trial != "train1",
#            trial != "train2")%>%
#   mutate(age = age_num - min(age_num))
# 
# sem_know_run_summary <- item_params%>%
#   spread(parameter,value)%>%
#   group_by(Chain,item)%>%
#   summarise(overall_int = estimate_mode(int),
#             overall_slope = estimate_mode(slope))
# 
# 
# sem_know_run_iteration <- item_params%>%
#   spread(parameter,value)
# 
# sem_know_chain <- sem_know_run_iteration %>%
#   left_join(sem_know_run_summary)
# 
# chain_comp <- sem_know_chain %>%
#   left_join(expand.grid(
#       age = unique(me_model_data$age),
#       item = unique(me_model_data$familiar)), .) %>%
#   mutate(sem_know_ci = plogis(int + slope * age),
#          sem_know = plogis(overall_int + overall_slope * age)) %>%
#   group_by(Chain,item, age) %>%
#   summarise(sem_know = estimate_mode(sem_know), 
#             sem_know_uci = hdi_upper(sem_know_ci),
#             sem_know_lci = hdi_lower(sem_know_ci))
# 
# ggplot(chain_comp, aes(x = age+2, y= sem_know, fill = factor(Chain)))+
#   geom_line(size = .3, aes(col = factor(Chain)))+
#   geom_ribbon(aes(ymin=sem_know_lci, ymax = sem_know_uci), alpha = .05)+
#   ylab("Semantic knowledge")+
#   xlab("Age")+
#   ylim(0,1)+
#   facet_wrap(~item, nrow = 3)+
#   theme_few()+
#   scale_colour_discrete()+
#   scale_fill_discrete()

#ggsave("../graphs/model_by_chain.pdf", width = 5, height = 4, scale = 1.5)
```

#### speaker optimality

```{r}
# me_model_data <- read_csv("../data/me.csv")%>%
#     filter(trial != "train1",
#            trial != "train2")%>%
#   mutate(age = age_num - min(age_num))
# 
# so_run_iteration <- global_params%>%
#   spread(parameter, value)
# 
# so_run_summary <- so_run_iteration%>%
#   group_by(Chain)%>%
#   summarise(overall_int = estimate_mode(int),
#             overall_slope = estimate_mode(slope))
# 
# so_run <- so_run_iteration %>%
#   left_join(so_run_summary)
# 
# so_run_comp <-so_run%>%
#   group_by(Chain,Iteration)%>%
#   tidyr::expand(int,slope,overall_int,overall_slope,age = unique(me_model_data$age))%>%
#   mutate(so_ci = int + slope * age,
#          so = overall_int + overall_slope * age) %>%
#   group_by(Chain, age) %>%
#   summarise(so = estimate_mode(so), 
#             so_uci = hdi_upper(so_ci),
#             so_lci = hdi_lower(so_ci))
# 
# ggplot(so_run_comp, aes(x = age+2, y= so, fill = factor(Chain)))+
#   geom_line(size = .4, aes(col = factor(Chain)))+
#   geom_ribbon(aes(ymin=so_lci, ymax = so_uci), alpha = .05)+
#   ylab("speaker optimality")+
#   xlab("Age")+
#   #facet_grid(~run)+
#   theme_few()+
#   scale_colour_discrete()+
#   scale_fill_discrete()
# 
# ggsave("../graphs/so_by_chain.pdf", width = 4, height = 3, scale = 1.5)
```


#### Correlations between parameters for semantic knowledge

```{r}
# sem_know_model_1<- webppl(
#   program_code = paste(rsaUtils, rsaModel, semStr , sep='\n'),
#   data = list(data = me_model_data),
#   data_var = "dataFromR",
#   model_var = "model",
#   chains = 10,
#   cores = 10,
#   inference_opts = list(method = "incrementalMH",
#                         samples = 4000,
#                         burn = 2000,
#                         verbose = T,
#                         lag = 1,
#                         verboseLag = 1000)
# )
# 
# sem_know_model_2<- webppl(
#   program_code = paste(rsaUtils, rsaModel, semStr , sep='\n'),
#   data = list(data = me_model_data),
#   data_var = "dataFromR",
#   model_var = "model",
#   chains = 10,
#   cores = 10,
#   inference_opts = list(method = "incrementalMH",
#                         samples = 4000,
#                         burn = 2000,
#                         verbose = T,
#                         lag = 1,
#                         verboseLag = 1000)
# )%>%
#   mutate(Chain = Chain +10)
# 
# sem_know_model_3<- webppl(
#   program_code = paste(rsaUtils, rsaModel, semStr , sep='\n'),
#   data = list(data = me_model_data),
#   data_var = "dataFromR",
#   model_var = "model",
#   chains = 10,
#   cores = 10,
#   inference_opts = list(method = "incrementalMH",
#                         samples = 4000,
#                         burn = 2000,
#                         verbose = T,
#                         lag = 1,
#                         verboseLag = 1000)
# )%>%
#   mutate(Chain = Chain +20)
# 
# 
# sem_know_cor_params <- bind_rows(
#   sem_know_model_1,
#   sem_know_model_2,
#   sem_know_model_3
# )%>%
#   saveRDS(.,"../saves/run_correlations.rds")
  
  
# sem_know_cor_params  <- readRDS("../saves/run_correlations.rds")%>%
#   separate(Parameter, into = c("parameter", "type","item"), sep = ",")%>%
#   filter(parameter == "parameter", type == "items")%>%
#   separate(value, into = c("int", "slope"),sep = ", ")%>%
#   mutate(int = as.numeric(str_remove_all(int, "[(c]")),
#          slope = as.numeric(str_remove(slope, "[)]")))%>%
#   gather(parameter, value, int, slope)
# 
# 
# sem_know_cor_params <- sem_know_cor_params%>%
#   filter(Chain != "15",
#          Chain != "1",
#          Chain != "30",
#          Chain != "6",
#          Chain != "5",
#          Chain != "7",
#          Chain != "3")

```


```{r}
ggplot(item_params%>%filter(parameter == "int"), aes(x = value))+
  geom_density(alpha = 0.3)+
  xlab("Parameter")+
  facet_grid(Chain~item, scales = 'free')+
  theme_few()

#ggsave("../graphs/item_by_chain.pdf", width = 15, height = 15, scale = 1.5)
```

```{r}
item_params_cor <- item_params %>%
  group_by(Chain,item, parameter)%>%
  summarise(mode = estimate_mode(value))
```

```{r}
library(corrr)
library(corrplot)

## correaltion at 2
cor_int_2 <- item_params_cor%>%
  spread(Chain, mode) %>%
  filter(parameter == "int")%>%
  select(-parameter)%>%
  ungroup()%>%
  select(-item)%>%
  correlate(method = "spearman")%>%
  gather(key, value)%>%
  filter(key !="rowname")%>%
  na.omit()%>%
  mutate(value = as.numeric(value),
         age = "2")

cor_int_3 <- item_params_cor%>%
  spread(parameter, mode)%>%
  mutate(mode = int + slope)%>%
  select(Chain, item, mode)%>%
  spread(Chain, mode)%>%
  ungroup()%>%
  select(-item)%>%
  correlate(method = "spearman")%>%
  gather(key, value)%>%
  filter(key !="rowname")%>%
  na.omit()%>%
  mutate(value = as.numeric(value),
         age = "3")

cor_int_4 <- item_params_cor%>%
  spread(parameter, mode)%>%
  mutate(mode = int + 2*slope)%>%
  select(Chain, item, mode)%>%
  spread(Chain, mode)%>%
  ungroup()%>%
  select(-item)%>%
  correlate(method = "spearman")%>%
  gather(key, value)%>%
  filter(key !="rowname")%>%
  na.omit()%>%
  mutate(value = as.numeric(value),
         age = "4")


cor_int_5 <- item_params_cor%>%
  spread(parameter, mode)%>%
  mutate(mode = int + 3*slope)%>%
  select(Chain, item, mode)%>%
  spread(Chain, mode)%>%
  ungroup()%>%
  select(-item)%>%
  correlate(method = "spearman")%>%
  gather(key, value)%>%
  filter(key !="rowname")%>%
  na.omit()%>%
  mutate(value = as.numeric(value),
         age = "5")


cor_hist <- 
bind_rows(
  cor_int_2,
  cor_int_3,
  cor_int_4,
  cor_int_5
)


ggplot(cor_hist, aes(x = value, fill =age))+
  geom_density(alpha = .5)+
  xlim(-1,1)+
  xlab("Correlation")+
  theme_few()


#ggsave("../graphs/corr_int_by_age.pdf", width = 5, height = 4, scale = 1.5)

```


# Inverse predictions

```{r}
# speak_opt_params <- speak_opt %>%
#   group_by(parameter)%>%
#   summarise(mode = estimate_mode(value),
#             uci = hdi_upper(value),
#             lci = hdi_lower(value))
# 

inv_model_data <- item_params_summary%>%
  select(-uci,-lci)%>%
  spread(parameter, mode)%>%
  tidyr::expand(item, int,slope,age = unique(me_model_data$age- min(me_model_data$age)))%>%
  mutate(so_int = global_params_summary%>%filter(token == "speaker_optimality", parameter == "int")%>%pull(mode),
         so_slope = global_params_summary%>%filter(token == "speaker_optimality", parameter == "slope")%>%pull(mode))%>%
  mutate(speaker_optimality = so_int + so_slope * age,
         semantic_knowledge = plogis(int + slope * age))

```


```{r}
invPred <- '
var allData = dataFromR.data

var priorProbs = [.5,.5]

var output = map(function(row){

    var modelPredictions = pragmaticListener({label: "novel_word"}, priorProbs, row.speaker_optimality,row.semantic_knowledge)

    return extend([row.item + "/" + row.age, Math.exp(modelPredictions.score(1))])

}, allData)

output


'
```


```{r predictions pragmatic model ex 3 adults}
inv_model_pred<- webppl(
  program_code = paste(rsaUtils, rsaModel, invPred , sep='\n'),
  data =list(data = inv_model_data),
  data_var = "dataFromR"
)
# 
# #saveRDS(inv_model_pred, "../saves/inv_model_pred.rds")
# #inv_model_pred <- readRDS("../saves/inv_model_pred.rds")
# 
inv_predictions <- inv_model_pred %>%
  separate(`0`, into = c("item", "age"), sep="/")%>%
  mutate(age = as.numeric(age))%>%
  rename( me_effect = `1`)%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         age = age + min(me_data$age_num))
```

```{r}
# plot_model_pred <- model_pred_summary%>%
#   left_join(aoa_ratings) %>%
#   ungroup()%>%
#   mutate(item = fct_reorder(factor(item), mean_aoa))

model_plot <- ggplot(inv_predictions, aes(x=age,y = me_effect, col = item))+
  geom_hline(yintercept = 0.5, lty=2)+
  geom_line(size = 1)+
  ggtitle("Inverse Model Predictions")+
  labs(x="Age",y="Proportion correct")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d()

```

```{r}
p2 <- read_csv("../data/me.csv")%>%
  filter(trial != "train1",
         trial != "train2")%>%
  mutate(item = familiar)%>%
    left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(familiar = fct_reorder(factor(item), mean_aoa))

data_plot <- ggplot(data = p2, aes(x = age_num, y = correct, col = familiar, fill = familiar)) +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_jitter(width = 0, height = 0.05, alpha = .6)+
  geom_smooth(method = "loess", se = F, alpha = .5, span = 2)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  #facet_grid(~familiar)+
  ggtitle("Data")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d(name = "familiar object")+
  scale_fill_viridis_d(name = "familiar object")

```
```{r}
ggarrange(data_plot,model_plot, common.legend = T, legend = "right", nrow = 1, ncol = 2)
```

## For comparison: GLMM for ME effect and data
```{r}
me_data <- read_csv("../data/me.csv")%>%
    filter(trial != "train1",
           trial != "train2")%>%
  mutate(z_age = age_num - mean(age_num))

# me_model <- brm(correct ~ z_age + (1|subid) + (z_age | familiar),
#                 data = me_data,
#                 family = bernoulli(),
#                 control = list(adapt_delta = 0.95),
#                 sample_prior = F,
#                 save_all_pars = TRUE,
#                 iter = 2000)

#saveRDS(me_model, "../saves/me_effect_model.rds")

me_model <- readRDS("../saves/me_effect_model.rds")

summary(me_model)

fixef <- as_tibble(fixef(me_model), rownames = "term")

ranef <-  ranef(me_model)

age_me_effect <- as_data_frame(ranef$familiar, rownames = "familiar")%>%
  mutate(grand_intercept = fixef%>%filter(term=="Intercept")%>%pull(Estimate),
         grand_slope = fixef%>%filter(term=="z_age")%>%pull(Estimate))%>%
  group_by(familiar) %>%
  tidyr::expand(Estimate.Intercept,Estimate.z_age,grand_intercept,grand_slope,z_age = me_data$age_num - mean(me_data$age_num), mean_age = mean(me_data$age_num))%>%
  mutate(me_effect = plogis(grand_intercept + Estimate.Intercept+(Estimate.z_age+grand_slope)*z_age), 
         age = z_age+mean_age,
         item = familiar)%>%
  select(item, me_effect, age)%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa))


glmm_plot <- ggplot(age_me_effect, aes(x=age,y = me_effect, col = item))+
  geom_hline(yintercept = 0.5, lty=2)+
  geom_line(size = 1)+
  ggtitle("GLMM")+
  labs(x="Age",y="Proportion correct")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d(name = "familiar object")



```


```{r}
ggarrange(data_plot,model_plot,glmm_plot, common.legend = T, legend = "right", nrow = 1, ncol = 3)
```

## Model predictions for combination

```{r}
# novelty data

novel_data <- read_csv("../data/novelty.csv")%>%
  mutate(age_num = age_num-min(age_num))

# novel_bm <- brm(correct ~ age_num + (1 | subid) + (age_num | agent), 
#           data = novel_data, family = bernoulli(),
#           control = list(adapt_delta = 0.95),
#           sample_prior = F,
#           save_all_pars = F,
#           iter = 10000)
# 
# saveRDS(novel_bm, "../saves/novelty_model.rds")

novel_bm <- readRDS("../saves/novelty_model.rds")

#summary(novel_bm)

novel_params <- as_tibble(fixef(novel_bm), rownames = "term")%>%
  select(term,Estimate)%>%
  spread(term, Estimate)

comb_model_data_con <- inv_model_data %>%
  mutate(nov_int = novel_params$Intercept,
         nov_slope = novel_params$age_num,
         prior = plogis(nov_int + age * nov_slope),
         alignment = "congruent")

  
comb_model_data_incon <- inv_model_data %>%
  mutate(nov_int = novel_params$Intercept,
         nov_slope = novel_params$age_num,
         prior = 1- plogis(nov_int + age * nov_slope),
         alignment = "incongruent")


comb_model_data <- bind_rows(
  comb_model_data_con,
  comb_model_data_incon)
```

```{r}
combPred <- '
var allData = dataFromR.data

var output = map(function(row){

    var prior = [row.prior, 1-row.prior]

    var modelPredictions = pragmaticListener({label: "novel_word"}, prior, row.speaker_optimality,row.semantic_knowledge)

    return extend([row.alignment + "/" +  row.item + "/" + row.age, Math.exp(modelPredictions.score(1))])

}, allData)

output


'
```


```{r predictions pragmatic model ex 3 adults}
comb_model_pred<- webppl(
  program_code = paste(rsaUtils, rsaModel, combPred , sep='\n'),
  data =list(data = comb_model_data),
  data_var = "dataFromR"
)


comb_model_predictions <- comb_model_pred %>%
  separate(`0`, into = c("alignment","item", "age"), sep="/")%>%
  mutate(age = as.numeric(age))%>%
  rename( me_effect = `1`)%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         age = age + min(me_data$age_num),
         source = "Model")

```


```{r}
p_comb_model <- ggplot(comb_model_predictions, aes(x=age,y = me_effect, col = item))+
  geom_hline(yintercept = 0.5, lty=2)+
  geom_line(size = 1)+
  ggtitle("Combination Model Predictions")+
  labs(x="Age",y="Proportion correct")+
  facet_grid(~alignment)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d()
```

```{r}

data_comb <- read_csv("../data/combination.csv") %>%
  filter(condition != "train")%>%
  mutate(alignment = condition,
         item = familiarObject)%>%
  left_join(aoa_ratings)%>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(familiarObject), mean_aoa), 
         source = "Data")

p_comb_data <- ggplot(data = data_comb, aes(x = age_num, y = correct, col = item, fill = item)) +
  geom_hline(yintercept = 0.5, lty=2)+
  geom_jitter(width = 0, height = 0.05, alpha = .6)+
  #geom_smooth(method = "glm", method.args = list(family = "binomial"), se = F, alpha = .5)+
  geom_smooth(method = "loess", se = F, alpha = .5, span = 2)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  facet_grid(~alignment)+
  ggtitle("Combination Data")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d()+
  scale_fill_viridis_d()
```

```{r}
ggarrange(p_comb_model,p_comb_data, common.legend = T, legend = "right", nrow = 2, ncol = 1)
```

```{r}
#ggsave("../graphs/combination_model_data.pdf", width = 7, height = 7, scale = 1)
```

```{r}
ggplot(data = data_comb, aes(x = age_num, y = correct, col = source, fill = source, lty = alignment)) +
  geom_hline(yintercept = 0.5, lty=2)+
  #geom_jitter(width = 0, height = 0.05, alpha = .6)+
  #geom_smooth(method = "glm", method.args = list(family = "binomial"), se = F, alpha = .5)+
  geom_smooth(method = "loess", se = F, alpha = .5, span = 2)+
  geom_line(data = comb_model_predictions, aes(x=age,y = me_effect, col = source, lty = alignment), size = 1)+
  labs(x="Age",y="Mutual Exclusivity effect")+
  #facet_grid(condition~familiarObject)+
  facet_grid(~item)+
  #ggtitle("Combination Data")+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_ptol(name = NULL)+
  scale_fill_ptol(name = NULL)
```


```{r}
ggsave("../graphs/combination_model_data_by_item.pdf", width = 8, height = 3, scale = 1.5)
```
# Correlate model predictions and data 
```{r}
binnned_data <- data_comb%>%
  group_by(subage, alignment, item)%>%
  summarize(k = sum(correct), n = n())%>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         data_mean = (a-1)/(a+b-2),
         data_ci_lower  = qbeta(.025, a, b),
         data_ci_upper = qbeta(.975, a, b),
         subage = factor(subage))%>%
  select(-a,-b,-n,-k)


binned_model <- comb_model_predictions %>%
  mutate(subage = cut(age, 
                      breaks = c(2,3,4,5),
                      labels = c(2,3,4)))%>%
  group_by(subage, alignment, item)%>%
  summarise(model_mean = estimate_mode(me_effect),
            model_ci_lower = hdi_lower(me_effect),
            model_ci_upper = hdi_upper(me_effect))


cor_plot <- left_join(
  binnned_data,
  binned_model)


cor_plot_age <- ggplot(data = cor_plot,aes(x = model_mean, y = data_mean, col = item)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_errorbar(aes(ymin = data_ci_lower, ymax = data_ci_upper),width = 0,size = .7)+
  geom_errorbarh(aes(xmin = model_ci_lower, xmax = model_ci_upper), height = 0,size = .7)+
  geom_point(size = 2,  stroke = 1)+
  coord_fixed()+
 stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = model_mean, y = data_mean), inherit.aes = F, size = 3)+
  xlim(0,1)+ylim(0,1)+
  xlab("Model")+
  ylab("Data")+
  facet_grid(~subage)+
  theme_few() + 
  scale_colour_viridis_d(name ="Item")
```

```{r}
#ggsave("../graphs/combination_correlation.pdf", width = 8, height = 4, scale = 1.2)
```


```{r}
ggarrange(
plot_item_params,plotg_subj_sigma,
plot_global_params,plot_subject_params,
cor_plot_age, plot_global_sigma,
ncol = 2, nrow = 3, common.legend = T, widths = c(1.7,1))

ggsave("../graphs/overview_wide_slope_wide_sigma_pos_slope_ind.png", width = 12, height = 10, scale = 1.2)


```



#### WORK IN PROGRESS: nesting within participant

```{r work in progress}
sem_subj_str <- '
var data = dataFromR.data

var priorProbs = [.5,.5]

var familiars = levels(data, "familiar")
var subjects = levels(data, "subid")

var model  = function(){

  var speakerOptimalityParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 4, width: 0.5})
  }

  var globalLineParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 2, width: 0.5})
  }

  var itemVariability = {
    intercept: uniformDrift({a: 0, b: 2, width: 0.2}),
    slope: uniformDrift({a: 0, b: 1, width: 0.2})
  }

  // conceptually it might not make sense to have two sigmas, but maybe it does
  // intercept sigma probably will be smaller than slope sigma
  // var item_int_sigma = uniformDrift({a: 0, b: 3, width: 0.2})
  
  var sampleItemParameters = function(itemName) { 
    return [itemName, {
        intercept: gaussianDrift({
          mu: globalLineParameters.intercept, 
          sigma: itemVariability.intercept, 
          width: 0.5
        }),
        slope: Math.pow(gaussianDrift({
          mu: globalLineParameters.slope, 
          sigma: itemVariability.slope, 
          width: 0.5
        }), 2)
      }]
  }

  // {papaya: {int, slope}, bread: {int, slope}, ... }
  var all_item_parameters = _.fromPairs(map(sampleItemParameters, familiars))
  
  // want something that favors smaller values
  var subject_sigma = uniformDrift({a: 0, b:1, width: 0.1}) 
  
  var sampleLinguisticCompetence = function(age){
    return gaussianDrift({ mu: age, sigma: subject_sigma, width: 0.1 })
  }

  foreach(function(subid){
      var subjectData = _.filter(data, {subid: subid})
      var subj_age = subjectData[0].age
      var subj_linguistic_competence = sampleLinguisticCompetence(subj_age)
      
     // display("age = " + subj_age + " ... intercept = " + subj_linguistic_competence)
  
      // question: should we use age here instead of the random intercept? or a different random intercept?
    
  var speakerOptimality = speakerOptimalityParameters.intercept  + speakerOptimalityParameters.slope * subj_age
      
      // each row is a different item
      foreach(function(row){

        var itemLineParameters = all_item_parameters[row.familiar]
        
        var sem_knowledge = logistic(itemLineParameters.intercept + 
            itemLineParameters.slope * subj_linguistic_competence)
  
        var modelPredictions = pragmaticListener({label: "novel_word"}, 
              priorProbs, speakerOptimality, sem_knowledge)
        // display(modelPredictions.score(row.correct))
        
        observe(modelPredictions, row.correct)
        
        // if you want to save predictions, make sure to record subid because these predictions are for this particular subid
        // query.add(["modelPrediction", cndtn, age], [age, Math.exp(modelPredictions.score(1))])

      }, subjectData)
    
    // comment out to save memory
    query.add(["parameter","subj_intercept", subid], [subjectData[0].age, subj_linguistic_competence])
      
  }, subjects)
  
  foreach(function(row){
      var prior = [row.prior, 1-row.prior]
      foreach(function(item){
        var itemLineParameters = all_item_parameters[item]
        var sem_knowledge = logistic(itemLineParameters.intercept + 
            itemLineParameters.slope * row.age)
        var speakerOptimality = speakerOptimalityParameters.intercept  + speakerOptimalityParameters.slope * row.age

        var modelPredictions = pragmaticListener({label: "novel_word"}, prior, speakerOptimality, sem_knowledge)

display(row.alignment)

        query.add(["prediction", item, row.age], [row.alignment, Math.exp(modelPredictions.score(1))])

      }, familiars)
   }, dataFromR.comb_data)

  foreach(function(item){
    var itemLineParameters = all_item_parameters[item]
    query.add(["parameter","items", item], [itemLineParameters.intercept, itemLineParameters.slope])
  }, familiars)

  query.add(["parameter","parameters","speaker_optimality"], [speakerOptimalityParameters.intercept, speakerOptimalityParameters.slope])
  query.add(["parameter","parameters", "global_sem"], [globalLineParameters.intercept, globalLineParameters.slope])
  query.add(["parameter","sigma", "global_sem_sigmas"], [itemVariability.intercept, itemVariability.slope])
  query.add(["parameter","sigma", "global_subject_sigmas"], ["NA", subject_sigma])
 
  return query 
}
'
```


```{r}

novel_bm <- readRDS("../saves/novelty_model.rds")

#summary(novel_bm)

novel_params <- as_tibble(fixef(novel_bm), rownames = "term")%>%
  select(term,Estimate)%>%
  spread(term, Estimate) 

me_model_data <- read_csv("../data/me.csv")%>%
    filter(trial != "train1",
           trial != "train2")%>%
  mutate(age = age_num - min(age_num),
         nov_int = novel_params$Intercept,
         nov_slope = novel_params$age_num,
         prior = plogis(nov_int + age * nov_slope))

comb_model_data <- bind_rows(
  me_model_data %>%
    distinct(age, prior) %>%
    mutate(alignment = "congruent"),
  me_model_data %>%
    distinct(age, prior) %>%
    mutate(prior = 1 - prior, 
         alignment = "incongruent")
)

sem_know_model<- webppl(
  program_code = paste(rsaUtils, rsaModel, sem_subj_str , sep='\n'),
  data = list(data = me_model_data, comb_data = comb_model_data),
  data_var = "dataFromR",
  model_var = "model",
  chains = 1,
  cores = 1,
  inference_opts = list(method = "incrementalMH",
                        samples = 10,
                        burn = 1,
                        verbose = T,
                        lag = 1,
                        verboseLag = 1000)
)

# run variants
# start with variant 3
# 3 cores, 3 chains 
# see how long it takes and then run over night 



# saveRDS(sem_know_model, "../saves/sem_know_model_save pred.rds")


```

#### plotting predictions

```{r}
comb_model_pred_agg <- model_pred%>%
  group_by(alignment,item,age)%>%
  summarise(mean = estimate_mode(me_effect),
            uci = hdi_upper(me_effect),
            lci = hdi_lower(me_effect))%>%
  left_join(aoa_ratings) %>%
  ungroup()%>%
  mutate(item = fct_reorder(factor(item), mean_aoa),
         age = age + min(me_data$age_num))

ggplot(comb_model_pred_agg, aes(x=age,y = mean, col = item))+
  geom_hline(yintercept = 0.5, lty=2)+
  geom_line(size = 1)+
  ggtitle("Combination Model Predictions")+
  labs(x="Age",y="Proportion correct")+
  facet_grid(~alignment)+
  theme_few() +
  ylim(-0.05,1.05)+
  xlim(2,5)+
  guides(alpha = F)+ 
  scale_colour_viridis_d()

```
