---
title: "model-understanding"
author: "MH Tessler"
date: "5/4/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Breaking down the model, for understanding

## Model utilities

```{r}
library(tidyverse)
library(viridis)
library(rwebppl)
library(brms)
library(coda)
library(ggthemes)
library(readxl)
library(ggpubr)
library(stringr)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}


hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}

me_data <- read_csv("../data/child_basic_me.csv")

```

## RSA model

```{r rsaUtils}
rsaUtils <- '
var all_objects = [
{ shape: "novel_object"},  
{ shape: "familiar_object"}
]

var labels = ["novel_word","familiar_word"]


var lexicon1 = function(utterance, obj, sem_knowledge){
utterance.label == "novel_word" ? obj.shape == "novel_object" :
utterance.label == "familiar_word" ? flip(sem_knowledge) ? obj.shape == "familiar_object" :  obj.shape == "novel_object" : 
true
}

var lexicon2 = function(utterance, obj, sem_knowledge){
utterance.label == "novel_word" ? obj.shape == "familiar_object" :
utterance.label == "familiar_word" ? flip(sem_knowledge) ? obj.shape == "familiar_object" :  obj.shape == "novel_object" : 
true
}

var lexiconObjects = {
"novel_word = novel_object": {
novel_object: "novel_word", familiar_object: "familiar_word"
},
"novel_word = familiar_object": {
novel_object: "familiar_word", familiar_object: "familiar_word"
},
}

var lexiconObject = {
"novel_word = novel_object": lexicon1,
"novel_word = familiar_object" : lexicon2
}

var utterancePrior = function(){ return uniformDraw([ {label: "novel_word"}, {label: "familiar_word"}]) }

var LexiconPrior = Categorical({vs: ["novel_word = novel_object","novel_word = familiar_object" ], ps: [1, 1]})

var foreach = function(fn, lst) {
    var foreach_ = function(i) {
        if (i < lst.length) {
            fn(lst[i]);
            foreach_(i + 1);
        }
    };
    foreach_(0);
};


var logistic = function(x) {1 / (1 + Math.exp(-x))}

var levels = function(df, label){
  return _.uniq(_.map(df, label));
}
'
```

```{r rsa model}
rsaModel <- '
var literalListener = cache(function(utterance, priorProbs, sem_knowledge){
  Infer({method: "enumerate", model: function(){
    var lexiconName = sample(LexiconPrior); 
    var lexicon = lexiconObject[lexiconName];
    var obj = sample( Categorical({vs: all_objects, ps: priorProbs}));
    if ("label" in utterance) {
      var truthValue = lexicon(utterance, obj, sem_knowledge);
      condition(truthValue)
    }
    return obj.shape 
  }})
}, 10000)

var speaker = cache(function(obj, lexiconName, priorProbs, speakerOptimality, sem_knowledge){
  Infer({method: "enumerate", model: function(){
    var utterance = utterancePrior();
    var L0 = literalListener(utterance, priorProbs, sem_knowledge);
    factor(speakerOptimality * L0.score(obj.shape))
    return utterance
  }})
}, 10000)

var pragmaticListener = cache(function(utterance, priorProbs, speakerOptimality, sem_knowledge){
  Infer({method: "enumerate", model: function(){
    var lexiconName = sample(LexiconPrior);
    var obj = sample( Categorical({vs: all_objects, ps: priorProbs}));
    var S1 = speaker(obj, lexiconName, priorProbs, speakerOptimality, sem_knowledge);
    observe(S1, utterance)
    return obj.shape == "novel_object" ? 1 : 0
  }})
}, 10000)

'
```



## Priors for speaker optimality

```{r FIXME}
semKnowPriors <- '
var data = dataFromR.data

var priorProbs = [.5,.5]

var familiars = levels(data, "familiar")

var model  = function(){

  var so_slope = uniformDrift({a: -2, b: 2, width: 0.2})
  var so_int = uniformDrift({a: -2, b: 2, width: 0.2})
  //var speakerOptimality = uniformDrift({a: -5, b: 5, width: 0.2})

  var sem_slope_global = uniformDrift({a: -2, b: 2, width: 0.2})
  var sem_int_global = uniformDrift({a: 0, b: 5, width: 0.2})

  var item_slope_sigma = uniformDrift({a: 0, b: 5, width: 0.2})
  // conceptually it might not make sense to have two sigmas, but maybe it does
  // intercept sigma probably will be smaller than slope sigma
  var item_int_sigma = uniformDrift({a: 0, b: 5, width: 0.2})
  
  foreach(function(cndtn){

    var conditionData = _.filter(data, {familiar: cndtn})
    
    var sem_slope_item = gaussianDrift({mu: sem_slope_global, sigma: item_slope_sigma, width: 0.5})
    var sem_int_item = gaussianDrift({mu: sem_int_global, sigma: item_int_sigma, width: 0.5})
    
    // var sem_slope = uniformDrift({a: -2, b: 2, width: 0.2})
    // var sem_int = uniformDrift({a: 0, b: 5, width: 0.2})

    foreach(function(row){

      var age = row.age

      //display(age)

      var sem_knowledge = logistic(sem_int_item + sem_slope_item * age)

      var speakerOptimality = so_int  + so_slope * age

      var modelPredictions = pragmaticListener({label: "novel_word"}, 
            priorProbs, speakerOptimality, sem_knowledge)
      // display(modelPredictions.score(row.correct))
      
      observe(modelPredictions, row.correct)
      
      query.add(["modelPrediction", cndtn, age], [age, Math.exp(modelPredictions.score(1))])

    }, conditionData)
    
    query.add(["parameter","items", cndtn], [sem_int_item, sem_slope_item])

  }, familiars)

  query.add(["parameter","parameters","speaker_optimality"], [so_int, so_slope])
  query.add(["parameter","parameters", "global_sem"], [sem_int_global, sem_slope_global])
  query.add(["parameter","sigma", "global_sem_sigmas"], [item_int_sigma, item_slope_sigma])
  
  return query 
}
'
```

## Priors for inferring semantic knowledge


```{r loop through bda model structure}
semKnowPriors <- '

var logistic = function(x) {1 / (1 + Math.exp(-x))}

var levels = function(df, label){
  return _.uniq(_.map(df, label));
}

var data = dataFromR.data

var priorProbs = [.5,.5]

var familiars = levels(data, "familiar")

var discreteUniformBins = function(lower, upper, bin_width){
  _.range(lower, upper, bin_width)
}

var all_ages = sort(levels(data, "age"))

var model  = function(){

  var globalLineParameters = {
    intercept:  discreteUniformBins(0, 5, 1),
    slope: discreteUniformBins(-2, 2.01, 1)
  }

  var itemVariability = {
    intercept: discreteUniformBins(0.01, 5, 1),
    slope: discreteUniformBins(0.01, 5, 1)
  }
  // conceptually it might not make sense to have two sigmas, but maybe it does
  // intercept sigma probably will be smaller than slope sigma
  

    
  // display([
  //  globalLineParameters.intercept, itemVariability.intercept,
  //  globalLineParameters.slope, itemVariability.slope,
  //  itemLineParameters.intercept, itemLineParameters.slope
  // ])
  var output = map(function(globalLineIntercept){
  display(globalLineIntercept)
    map(function(globalLineSlope){
      map(function(itemVariabilityIntercept){
          map(function(itemVariabilitySlope){
          
            var itemLineParameters = {
              intercept: discreteUniformBins(
                globalLineIntercept - 2*itemVariabilityIntercept,
                 globalLineIntercept + 2*itemVariabilityIntercept,
                 itemVariabilityIntercept
              ),
              slope: discreteUniformBins(
                 globalLineSlope - 2*itemVariabilitySlope,
                globalLineSlope + 2*itemVariabilitySlope,
                itemVariabilitySlope
              )
            }
            
            map(function(itemLineIntercept){
                map(function(itemLineSlope){
                  map(function(age){
                    var sem_knowledge = logistic(
                      itemLineIntercept + itemLineSlope * age
                    )

                    var priorProbs =  [.5,.5]
                    var speakerOptimality = 1
                    
                    var modelPredictions = pragmaticListener({label: "novel_word"}, 
                            priorProbs, speakerOptimality, sem_knowledge)

                    return {
                      global_intercept: globalLineIntercept,
                      global_slope: globalLineSlope,
                      sigma_intercept:itemVariabilityIntercept, 
                      sigma_slope:itemVariabilitySlope, 
                      item_intercept: itemLineIntercept,
                      item_slope: itemLineSlope,
                      age: age,
                      sem_knowledge : sem_knowledge,
                      L1 : Math.exp(modelPredictions.score(1))
                    }
                
                  }, all_ages)
    
              }, itemLineParameters.slope)
            }, itemLineParameters.intercept)
            
        }, itemVariability.slope)
      }, itemVariability.intercept)
    }, globalLineParameters.slope)
  }, globalLineParameters.intercept)
  
  _.flattenDeep(output)
}
'
```


```{r}
me_model_data <- read_csv("../data/child_basic_me.csv")%>%
    filter(trial != "train1",
           trial != "train2")%>%
  mutate(age = age_num - min(age_num))

sem_know_model<- webppl(
  program_code = paste(rsaUtils, rsaModel, semKnowPriors , "model()", sep='\n'),
  data = list(data = me_model_data),
  data_var = "dataFromR",
  model_var = "model",
  #inference_opts = list(method = "enumerate")
)


subset.ages <- unique(sem_know_model$age)[seq(1, 90, 10)]
```


Parameter ranges

```{r}
summary(sem_know_model)
```

- Is it expected that L1 predicitons should only range between 0.25 and 0.75? (this may be because speaker optimality = 1)

### L1 prediction vs. sem_knowledge

```{r}
sem_know_model %>% 
  distinct(sem_knowledge, L1) %>%
  ggplot(., aes( x = sem_knowledge, y = L1 ))+
  geom_point()+
  ylim(0, 1)
```


### L1 prediction vs. item intercept

```{r}
sem_know_model %>% 
  distinct(item_intercept, L1) %>%
  ggplot(., aes( x = item_intercept, y = L1 ))+
  geom_point()+
  ylim(0, 1)
```

### L1 prediction vs. item slope

```{r}
sem_know_model %>% 
  distinct(item_slope, L1) %>%
  ggplot(., aes( x = item_slope, y = L1 ))+
  geom_point()+
  ylim(0, 1)
```

#### Original priors entailed the effective range for item parameters of [-10, 10] for the slope and [-8, 8] for the intercept


```{r loop through relevant range}
semKnowPriorsReduced <- '

var logistic = function(x) {1 / (1 + Math.exp(-x))}

var levels = function(df, label){
  return _.uniq(_.map(df, label));
}

var data = dataFromR.data

var priorProbs = [.5,.5]

var familiars = levels(data, "familiar")

var discreteUniformBins = function(lower, upper, bin_width){
  _.range(lower, upper, bin_width)
}

var all_ages = sort(levels(data, "age"))

var model  = function(){
  var itemLineParameters = {
    intercept: discreteUniformBins( -8, 9, 1),
    slope: discreteUniformBins( -10, 11, 1 )
  }
  var output = map(function(itemLineIntercept){
                map(function(itemLineSlope){
                  map(function(age){
                    var sem_knowledge = logistic(
                      itemLineIntercept + itemLineSlope * age
                    )

                    var priorProbs =  [.5,.5]
                    var speakerOptimality = 1
                    
                    var modelPredictions = pragmaticListener({label: "novel_word"}, 
                            priorProbs, speakerOptimality, sem_knowledge)

                    return {
                      item_intercept: itemLineIntercept,
                      item_slope: itemLineSlope,
                      age: age,
                      sem_knowledge : sem_knowledge,
                      L1 : Math.exp(modelPredictions.score(1))
                    }
                
                  }, all_ages)
    
              }, itemLineParameters.slope)
            }, itemLineParameters.intercept)
  
  _.flattenDeep(output)
}
'
```


```{r}

sem_know_model_reduced <- webppl(
  program_code = paste(rsaUtils, rsaModel, semKnowPriorsReduced , "model()", sep='\n'),
  data = list(data = me_model_data),
  data_var = "dataFromR",
  model_var = "model",
  #inference_opts = list(method = "enumerate")
)


subset.ages <- unique(sem_know_model$age)[seq(1, 90, 10)]
```

### L1 prediction vs. item slope X item intercept

```{r}
sem_know_model_reduced %>% 
  filter(age %in% subset.ages) %>%
  filter(item_intercept < 5, item_intercept > -5,
         item_slope < 5, item_slope > -5) %>%
  distinct(age, item_intercept, item_slope, sem_knowledge) %>%
  ggplot(., aes( x = item_intercept, y = item_slope, fill = sem_knowledge ))+
  geom_tile()+
  facet_wrap(~age)+
  scale_fill_viridis()

ggsave("semKnowledge_bySlope_int.pdf")
```
