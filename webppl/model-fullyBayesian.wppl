// example call: time webppl model-fullyBayesian.wppl --require webppl-json --require webppl-sample-writer 1
// ~/webppl-github/webppl model.wppl --require webppl-json --require webppl-sample-writer $SLURM_ARRAY_TASK_ID
var chain = last(process.argv) // load index as last command line index

var all_objects = [{
    shape: "novel_object"
  },
  {
    shape: "familiar_object"
  }
]

var labels = ["novel_word", "familiar_word"]


var lexicon1 = function(utterance, obj, sem_knowledge) {
  utterance.label == "novel_word" ? obj.shape == "novel_object" :
    utterance.label == "familiar_word" ? flip(sem_knowledge) ?
    obj.shape == "familiar_object" :
    flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" :
    true
}

var lexicon2 = function(utterance, obj, sem_knowledge) {
  // display(sem_knowledge)
  utterance.label == "novel_word" ? obj.shape == "familiar_object" :
    utterance.label == "familiar_word" ? flip(sem_knowledge) ?
    obj.shape == "familiar_object" :
    flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" :
    true
}

var lexiconObjects = {
  "novel_word = novel_object": {
    novel_object: "novel_word",
    familiar_object: "familiar_word"
  },
  "novel_word = familiar_object": {
    novel_object: "familiar_word",
    familiar_object: "familiar_word"
  },
}

var lexiconObject = {
  "novel_word = novel_object": lexicon1,
  "novel_word = familiar_object": lexicon2
}

var utterancePrior = function() {
  return uniformDraw([{
    label: "novel_word"
  }, {
    label: "familiar_word"
  }])
}

var LexiconPrior = Categorical({
  vs: ["novel_word = novel_object", "novel_word = familiar_object"],
  ps: [1, 1]
})

var addNoise = function(dist, noiseParam) {
  Infer({
    model: function() {
      return flip(noiseParam) ? uniformDraw([0, 1]) : sample(dist)
    }
  })
}


var foreach = function(fn, lst) {
  var foreach_ = function(i) {
    if (i < lst.length) {
      fn(lst[i]);
      foreach_(i + 1);
    }
  };
  foreach_(0);
};

var logistic = function(x) {
  1 / (1 + Math.exp(-x))
}

var levels = function(df, label) {
  return _.uniq(_.map(df, label));
}

//////////////// Inferring parameters //////////////

var meData = json.read('../data/me.json');
var priorData = json.read('../data/novelty.json');
var combData = json.read('../data/combination.json');

var priorSubjects = levels(priorData, "subid")
var priorSubjectsAges = sort(levels(priorData, "age_month"))
var familiars = levels(meData, "item")
var familiarsAges = sort(levels(meData, "age_month"))
// var meDataAges = levels
var subjects = levels(meData, "subid")
var combDataAges = sort(levels(combData, "age_month"))

var priorProbs = [.5, .5]

// _.uniq(priorSubjectsAges.concat(familiarsAges).concat(combDataAges)).length
// combDataAges.length
// combData
var model = function() {

  ////////////// Prior ////////////////////////

  var prior_slope = uniformDrift({
    a: -2,
    b: 2,
    width: 0.4
  })
  var prior_int = uniformDrift({
    a: -2,
    b: 2,
    width: 0.4
  })


  // random slopes
  // var prior_subject_sigma = uniformDrift({a: 0, b:3, width: 0.2})
  // var priorSampleSub = function(age){
  //   return gaussianDrift({ mu: age, sigma: prior_subject_sigma, width: 0.1 })
  // }

  foreach(function(age_month) {
    var priorSubjectDataByAge = _.filter(priorData, {
      age_month: age_month
    })
    // display(age_month + ' n data points = ' + priorSubjectDataByAge.length)
    var subj_age = priorSubjectDataByAge[0].age_month
    var priorSubjectDataByAge_correct = _.map(priorSubjectDataByAge, "correct")

    // random slopes (should be: by-subject...)
    // var subjectPrior = priorSampleSub(subj_age)

    var priorReg = logistic(prior_int + prior_slope * subj_age)
    var prior = [priorReg, 1 - priorReg]
    var modelPredictions = Infer({
      method: "enumerate",
      model: function() {
        var obj = sample(Categorical({
          vs: all_objects,
          ps: prior
        }));
        return obj.shape == "novel_object" ? 1 : 0
      }
    })

    mapData({
      data: priorSubjectDataByAge_correct
    }, function(d) {
      // display(modelPredictions.score(d))
      observe(modelPredictions, d);
    })

  }, priorSubjectsAges)

  query.add(["parameter","parameters", "prior", "intercept", "NA", "NA"], prior_int)
  query.add(["parameter","parameters", "prior", "slope", "NA", "NA"], prior_slope)
  // query.add(["parameter","sigma", "prior", "NA", "NA", "NA"], ["NA", prior_subject_sigma])


  //////////////// Semantic knowledge and speaker optimality ////////////////////////

  var speakerOptimalityParameters = {
    intercept: uniformDrift({
      a: -3,
      b: 3,
      width: 0.5
    }),
    slope: uniformDrift({
      a: 0,
      b: 4,
      width: 0.5
    })
  }

  var globalLineParameters = {
    intercept: uniformDrift({
      a: -3,
      b: 3,
      width: 0.5
    }),
    slope: uniformDrift({
      a: 0,
      b: 2,
      width: 0.5
    })
  }

  var itemVariability = {
    intercept: uniformDrift({
      a: 0,
      b: 2,
      width: 0.2
    }),
    slope: uniformDrift({
      a: 0,
      b: 1,
      width: 0.2
    })
  }

  // conceptually it might not make sense to have two sigmas, but maybe it does
  // intercept sigma probably will be smaller than slope sigma
  // var item_int_sigma = uniformDrift({a: 0, b: 3, width: 0.2})

  var sampleItemParameters = function(itemName) {
    return [itemName, {
      intercept: gaussianDrift({
        mu: globalLineParameters.intercept,
        sigma: itemVariability.intercept,
        width: 0.5
      }),
      slope: gaussianDrift({
        mu: globalLineParameters.slope,
        sigma: itemVariability.slope,
        width: 0.5
      })
    }]
  }

  // {papaya: {int, slope}, bread: {int, slope}, ... }
  var all_item_parameters = _.fromPairs(map(sampleItemParameters, familiars))

  // want something that favors smaller values
  var subject_sigma = uniformDrift({
    a: 0,
    b: 1,
    width: 0.1
  })

  var sampleLinguisticCompetence = function(age) {
    return gaussianDrift({
      mu: age,
      sigma: subject_sigma,
      width: 0.1
    })
  }

  foreach(function(age_month) {
    var subjectData_byAge = _.filter(meData, {
      age_month: age_month
    })

    var subj_age = subjectData_byAge[0].age_month
    var subj_linguistic_competence = subj_age //sampleLinguisticCompetence(subj_age)
    // display("age = " + subj_age + " ... intercept = " + subj_linguistic_competence)
    // question: should we use age here instead of the random intercept? or a different random intercept?
    var speakerOptimality = speakerOptimalityParameters.intercept + speakerOptimalityParameters.slope * subj_age

    foreach(function(item) {
      var subjectData_byAgeItem = _.filter(subjectData_byAge, {
        item: item
      })
      var subjectDataByAgeItem_correct = _.map(subjectData_byAgeItem, "correct")
      // display(age_month + ', item = ' + item +' n data points = ' + subjectData_byAgeItem.length)

      var itemLineParameters = all_item_parameters[item]

      var sem_knowledge = logistic(itemLineParameters.intercept +
        itemLineParameters.slope * subj_linguistic_competence)

      var literalListener = cache(function(utterance) {
        Infer({
          method: "enumerate",
          model: function() {
            var lexiconName = sample(LexiconPrior);
            var lexicon = lexiconObject[lexiconName];
            var obj = sample(Categorical({
              vs: all_objects,
              ps: [.5, .5]
            }));
            if ("label" in utterance) {
              var truthValue = lexicon(utterance, obj, sem_knowledge);
              condition(truthValue)
            }
            return obj.shape
          }
        })
      }, 10000)

      var speaker = cache(function(obj, lexiconName) {
        Infer({
          method: "enumerate",
          model: function() {
            var utterance = utterancePrior();
            var L0 = literalListener(utterance);
            factor(speakerOptimality * L0.score(obj.shape))
            return utterance
          }
        })
      }, 10000)

      var pragmaticListener = function(utterance) {
        Infer({
          method: "enumerate",
          model: function() {
            // display('inside RSA = ' + sem_knowledge)
            var lexiconName = sample(LexiconPrior);
            var obj = sample(Categorical({
              vs: all_objects,
              ps: [.5, .5]
            }));
            var S1 = speaker(obj, lexiconName);
            observe(S1, utterance)
            return obj.shape == "novel_object" ? 1 : 0
          }
        })
      }

      var modelPredictions = pragmaticListener({
        label: "novel_word"
      })

      mapData({
        data: subjectDataByAgeItem_correct
      }, function(d) {
        observe(modelPredictions, d)
      })

    }, familiars)

    // each row is a different item
    // foreach(function(row){

    // var itemLineParameters = all_item_parameters[row.item]


    // display('outside RSA = ' + sem_knowledge
    // display(sem_knowledge)

    // display(modelPredictions.score(row.correct))


    // if you want to save predictions, make sure to record subid because these predictions are for this particular subid
    // query.add(["modelPrediction", cndtn, age], [age, Math.exp(modelPredictions.score(1))])

    // }, subjectData)

    // comment out to save memory
    //  query.add(["parameter","subj_intercept", subid], [subjectData[0].age, subj_linguistic_competence])

  }, familiarsAges)


  //////////////// Model predictions and combination ////////////////////////

  var noise = uniformDrift({
    a: 0,
    b: 1,
    width: 0.1
  })

  foreach(function(age_month) {
    // display(age_month)
    var combData_byAge = _.filter(combData, {
      age_month: age_month
    })

    var priorReg = logistic(prior_int + prior_slope * age_month)

    var global_sem_knowledge = logistic(globalLineParameters.intercept +
      globalLineParameters.slope * age_month)

    var speakerOptimality = speakerOptimalityParameters.intercept +
      speakerOptimalityParameters.slope * age_month

    foreach(function(item) {
      // display(item)
      var itemLineParameters = all_item_parameters[item]
      var item_sem_knowledge = logistic(itemLineParameters.intercept +
        itemLineParameters.slope * age_month)


      foreach(function(alignment_condition) {
        // display(alignment_condition)
        var priorComb = (alignment_condition == "congruent") ? [priorReg, 1 - priorReg] : [1 - priorReg, priorReg]
        var combinationData_byAge_byItem_byCondition = _.filter(combData, {
          age_month: age_month,
          item: item,
          alignment: alignment_condition
        })

        // combinationData_byAge_byItem_byCondition.length == 0 ? display("WARNING: NO DATA IN SUBSET = " + item + " " + age_month + " " + alignment_condition) : null

        foreach(function(model_type) {

          // display(model_type)
          var sem_knowledge = item_sem_knowledge
          var priorProbs = priorComb

          var literalListener = cache(function(utterance) {
            Infer({
              method: "enumerate",
              model: function() {
                var lexiconName = sample(LexiconPrior);
                var lexicon = lexiconObject[lexiconName];
                var obj = sample(Categorical({
                  vs: all_objects,
                  ps: [.5, .5]
                }));
                if ("label" in utterance) {
                  var truthValue = lexicon(utterance, obj, sem_knowledge);
                  condition(truthValue)
                }
                return obj.shape
              }
            })
          }, 10000)

          var speaker = cache(function(obj, lexiconName) {
            Infer({
              method: "enumerate",
              model: function() {
                var utterance = utterancePrior();
                var L0 = literalListener(utterance);
                factor(speakerOptimality * L0.score(obj.shape))
                return utterance
              }
            })
          }, 10000)

          var pragmaticListener = function(utterance) {
            Infer({
              method: "enumerate",
              model: function() {
                // display('inside RSA = ' + sem_knowledge)
                var lexiconName = sample(LexiconPrior);
                var obj = sample(Categorical({
                  vs: all_objects,
                  ps: priorProbs
                }));
                var S1 = speaker(obj, lexiconName);
                observe(S1, utterance)
                return obj.shape == "novel_object" ? 1 : 0
              }
            })
          }

          var modelPredictions = pragmaticListener({
            label: "novel_word"
          })
          var combinationData_byAge_byItem_byCondition_responses = _.map(combinationData_byAge_byItem_byCondition, "correct")

          mapData({
            data: combinationData_byAge_byItem_byCondition_responses
          }, function(d) {
            modelPredictions.score(d) == -Infinity ? display("WARNING: DATA IS UNDEFINED UNDER MODEL") : null
            observe(modelPredictions, d)
          })

          // var noisyModelPredictions = addNoise(modelPredictions, noise)

          // display(expectation(modelPredictions))
          // display(expectation(noisyModelPredictions))
          query.add(["modelPrediction",model_type,"free", alignment_condition, item, age_month], Math.exp(modelPredictions.score(1)))

          // query.add(["modelPrediction",model_type,"noise", alignment_condition, item, "NA"], [age_month, Math.exp(noisyModelPredictions.score(1))])

        }, ["pragmatic"])

        // var priorOnlyModelPredictions = Infer({method: "enumerate", model: function(){
        //     var obj = sample( Categorical({vs: all_objects, ps: priorComb}));
        //     return obj.shape == "novel_object" ? 1 : 0
        //  }})

        // var noisyPriorOnlyModelPredictions = addNoise(priorOnlyModelPredictions, noise)
        // query.add(["modelPrediction","prior","free", alignment_condition, item, "NA"], [age_month, Math.exp(priorOnlyModelPredictions.score(1))])
        // query.add(["modelPrediction","prior","noise", alignment_condition, item, "NA"], [age_month, Math.exp(noisyPriorOnlyModelPredictions.score(1))])

        // display(expectation(priorOnlyModelPredictions))
        // display(expectation(noisyPriorOnlyModelPredictions))

      }, ["congruent", "incongruent"])

    }, familiars)

  }, combDataAges)


  foreach(function(item){
    var itemLineParameters = all_item_parameters[item]
    query.add(["parameter","items", item, "intercept", "NA", "NA"], itemLineParameters.intercept)
    query.add(["parameter","items", item, "slope", "NA", "NA"], itemLineParameters.slope)
  }, familiars)

  query.add(["parameter","parameters","speaker_optimality", "intercept", "NA", "NA"], speakerOptimalityParameters.intercept)
  query.add(["parameter","parameters","speaker_optimality", "slope", "NA", "NA"], speakerOptimalityParameters.slope)
  query.add(["parameter","parameters", "global_sem", "intercept", "NA", "NA"], globalLineParameters.intercept)
  query.add(["parameter","parameters", "global_sem", "slope", "NA", "NA"], globalLineParameters.slope)
  query.add(["parameter","sigma", "global_sem_sigmas", "intercept", "NA", "NA"], itemVariability.intercept)
  query.add(["parameter","sigma", "global_sem_sigmas", "slope", "NA", "NA"], itemVariability.slope)
  // query.add(["parameter","sigma", "global_subject_sigmas", "NA", "NA", "NA"], ["NA", subject_sigma])

  return query
}


var header = "iteration,a,b,c,d,e,f,g,h"

// var randomNumberForFile = Math.round(uniform(0, 1000))

var totalIterations = 100000, lag =  5;
// var totalIterations = 50000, lag =  5;
var samples = totalIterations/lag, burn = totalIterations / 2;

//var totalIterations = 100
//var samples = totalIterations
//var burn = 0
//var lag = 0
var output_file = 'output/samples-fullyBayesian-pragmatic-reFactor-noRandomEffects_mcmc-' + totalIterations + '_burn' + burn + '_lag' + lag + '_chain' + chain + '.csv'
var callback = webpplSampleWriter.streamQueryCSV(output_file, header);

var output = Infer({
  model,
  samples: samples,
  burn: burn,
  lag: lag,
  method: 'MCMC',
  // method: 'incrementalMH',
  onlyMAP: true,
  verbose: T,
  verboseLag: totalIterations / 20,
  callbacks: [callback]
});

'output written to ' + output_file

// combData
