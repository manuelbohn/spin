// example call: webppl model.wppl --require webppl-json --require webppl-sample-writer 1
// ~/webppl-github/webppl model.wppl --require webppl-json --require webppl-sample-writer $SLURM_ARRAY_TASK_ID
var chain = last(process.argv) // load index as last command line index

var all_objects = [
{ shape: "novel_object"},
{ shape: "familiar_object"}
]

var labels = ["novel_word","familiar_word"]


var lexicon1 = function(utterance, obj, sem_knowledge){
  utterance.label == "novel_word" ? obj.shape == "novel_object" :
  utterance.label == "familiar_word" ? flip(sem_knowledge) ?
    obj.shape == "familiar_object" :
  flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" :
  true
}

var lexicon2 = function(utterance, obj, sem_knowledge){
  // display(sem_knowledge)
  utterance.label == "novel_word" ? obj.shape == "familiar_object" :
  utterance.label == "familiar_word" ? flip(sem_knowledge) ?
    obj.shape == "familiar_object" :
  flip() ? obj.shape == "familiar_object" : obj.shape == "novel_object" :
  true
}

var lexiconObjects = {
"novel_word = novel_object": {
novel_object: "novel_word", familiar_object: "familiar_word"
},
"novel_word = familiar_object": {
novel_object: "familiar_word", familiar_object: "familiar_word"
},
}

var lexiconObject = {
"novel_word = novel_object": lexicon1,
"novel_word = familiar_object" : lexicon2
}

var utterancePrior = function(){ return uniformDraw([ {label: "novel_word"}, {label: "familiar_word"}]) }

var LexiconPrior = Categorical({vs: ["novel_word = novel_object","novel_word = familiar_object" ], ps: [1, 1]})

var addNoise = function(dist, noiseParam){
   Infer({model: function(){
      return flip(noiseParam) ? uniformDraw([0, 1]) : sample(dist)
    }
   })
}


var foreach = function(fn, lst) {
    var foreach_ = function(i) {
        if (i < lst.length) {
            fn(lst[i]);
            foreach_(i + 1);
        }
    };
    foreach_(0);
};

var logistic = function(x) {1 / (1 + Math.exp(-x))}

var levels = function(df, label){
  return _.uniq(_.map(df, label));
}

//////////////// Inferring parameters //////////////

var meData = json.read('../data/me.json');
var priorData = json.read('../data/novelty.json');
var combData = json.read('../data/combination.json');

var priorSubjects = levels(priorData, "subid")

var familiars = levels(meData, "item")

var subjects = levels(meData, "subid")

var priorProbs = [.5,.5]

var model  = function(){


    var literalListener = cache(function(utterance, sem_knowledge, priorProbs){
      Infer({method: "enumerate", model: function(){
        var lexiconName = sample(LexiconPrior);
        var lexicon = lexiconObject[lexiconName];
        var obj = sample( Categorical({vs: all_objects, ps: [.5,.5]}));
        if ("label" in utterance) {
          var truthValue = lexicon(utterance, obj, sem_knowledge);
          condition(truthValue)
        }
        return obj.shape
      }})
    }, 10000)

    var speaker = cache(function(obj, lexiconName, speakerOptimality, sem_knowledge, priorProbs){
      Infer({method: "enumerate", model: function(){
        var utterance = utterancePrior();
        var L0 = literalListener(utterance, sem_knowledge, priorProbs);
        factor(speakerOptimality * L0.score(obj.shape))
        return utterance
      }})
    }, 10000)

    var pragmaticListener = cache(function(utterance, speakerOptimality, sem_knowledge, priorProbs){
      Infer({method: "enumerate", model: function(){
        // display('inside RSA = ' + sem_knowledge)
        var lexiconName = sample(LexiconPrior);
        var obj = sample( Categorical({vs: all_objects, ps: priorProbs}));
        var S1 = speaker(obj, lexiconName, speakerOptimality, sem_knowledge, priorProbs);
        observe(S1, utterance)
        return obj.shape == "novel_object" ? 1 : 0
      }})
    }, 10000)


//////////////// Prior ////////////////////////

 var prior_slope = uniformDrift({a: -2, b: 2, width: 0.4})
  var prior_int = uniformDrift({a: -2, b: 2, width: 0.4})

  var prior_subject_sigma = uniformDrift({a: 0, b:3, width: 0.2})

  var priorSampleSub = function(age){
    return gaussianDrift({ mu: age, sigma: prior_subject_sigma, width: 0.1 })
  }

  foreach(function(subid){
      var priorSubjectData = _.filter(priorData, {subid: subid})

      //display(subjects)

      var subj_age = priorSubjectData[0].age

      var subjectPrior = priorSampleSub(subj_age)

    foreach(function(row){

      var priorReg = logistic(prior_int + prior_slope * subjectPrior)

      var prior = [priorReg, 1-priorReg]

      var modelPredictions = Infer({method: "enumerate", model: function(){
      var obj = sample( Categorical({vs: all_objects, ps: prior}));
      return obj.shape == "novel_object" ? 1 : 0
       }})

     observe(modelPredictions, row.correct)

    }, priorSubjectData)

  }, priorSubjects)

  query.add(["parameter","parameters", "prior", "NA", "NA", "NA"], [prior_int, prior_slope])
  query.add(["parameter","sigma", "prior", "NA", "NA", "NA"], ["NA", prior_subject_sigma])


//////////////// Semantic knowledge and speaker optimality ////////////////////////

  var speakerOptimalityParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 4, width: 0.5})
  }

  var globalLineParameters = {
    intercept: uniformDrift({a: -3, b: 3, width: 0.5}),
    slope: uniformDrift({a: 0, b: 2, width: 0.5})
  }

  var itemVariability = {
    intercept: uniformDrift({a: 0, b: 2, width: 0.2}),
    slope: uniformDrift({a: 0, b: 1, width: 0.2})
  }

  // conceptually it might not make sense to have two sigmas, but maybe it does
  // intercept sigma probably will be smaller than slope sigma
  // var item_int_sigma = uniformDrift({a: 0, b: 3, width: 0.2})

  var sampleItemParameters = function(itemName) {
    return [itemName, {
        intercept: gaussianDrift({
          mu: globalLineParameters.intercept,
          sigma: itemVariability.intercept,
          width: 0.5
        }),
        slope: gaussianDrift({
          mu: globalLineParameters.slope,
          sigma: itemVariability.slope,
          width: 0.5
        })
      }]
  }

  // {papaya: {int, slope}, bread: {int, slope}, ... }
  var all_item_parameters = _.fromPairs(map(sampleItemParameters, familiars))

    // want something that favors smaller values
  var subject_sigma = uniformDrift({a: 0, b:1, width: 0.1})

  var sampleLinguisticCompetence = function(age){
    return gaussianDrift({ mu: age, sigma: subject_sigma, width: 0.1 })
  }

  foreach(function(subid){
      var subjectData = _.filter(meData, {subid: subid})
      var subj_age = subjectData[0].age
      var subj_linguistic_competence = sampleLinguisticCompetence(subj_age)

     // display("age = " + subj_age + " ... intercept = " + subj_linguistic_competence)

      // question: should we use age here instead of the random intercept? or a different random intercept?

  var speakerOptimality = speakerOptimalityParameters.intercept  + speakerOptimalityParameters.slope * subj_age

      // each row is a different item
      foreach(function(row){

        var itemLineParameters = all_item_parameters[row.item]

        var sem_knowledge = logistic(itemLineParameters.intercept +
            itemLineParameters.slope * subj_linguistic_competence)
            // display('outside RSA = ' + sem_knowledge)

        // display(sem_knowledge)

        var modelPredictions = pragmaticListener({label: "novel_word"},
            speakerOptimality, sem_knowledge, priorProbs)
        // display(modelPredictions.score(row.correct))

        observe(modelPredictions, row.correct)

        // if you want to save predictions, make sure to record subid because these predictions are for this particular subid
        // query.add(["modelPrediction", cndtn, age], [age, Math.exp(modelPredictions.score(1))])

      }, subjectData)

    // comment out to save memory
  //  query.add(["parameter","subj_intercept", subid], [subjectData[0].age, subj_linguistic_competence])

  }, subjects)


//////////////// Model predictions and combination ////////////////////////


var noise = uniformDrift({a: 0, b:1, width: 0.1})

var all_log_likes = map(function(row) {

  var priorReg = logistic(prior_int + prior_slope * row.age)

  var cond = row.condition

  var prior = function(cnd){
    var cndPrior = cnd == "congruent" ? [priorReg, 1-priorReg] : [1-priorReg, priorReg] ;
    return cndPrior
  }

  var priorComb = prior(cond)

  var itemLineParameters = all_item_parameters[row.item]

  var sem_knowledge = logistic(itemLineParameters.intercept +
                               itemLineParameters.slope * row.age)

  var global_sem_knowledge = logistic(globalLineParameters.intercept +
                               globalLineParameters.slope * row.age)

  var speakerOptimality = speakerOptimalityParameters.intercept  +
      speakerOptimalityParameters.slope * row.age

  var pragModelPredictions = pragmaticListener({label: "novel_word"}, speakerOptimality,
                                               sem_knowledge, priorComb)

  var noisyPragModelPredictions = addNoise(pragModelPredictions, noise)



  var globalPragModelPredictions = pragmaticListener({label: "novel_word"},
                                               speakerOptimality,
                                               global_sem_knowledge, priorComb)

   var noisyGlobalPragModelPredictions = addNoise(globalPragModelPredictions, noise)


   var flatModelPredictions = pragmaticListener({label: "novel_word"},
                                               speakerOptimality,
                                               sem_knowledge, [.5, .5])

   var noisyFlatModelPredictions = addNoise(flatModelPredictions, noise)


  var priorOnlyModelPredictions = Infer({method: "enumerate", model: function(){
      var obj = sample( Categorical({vs: all_objects, ps: priorComb}));
      return obj.shape == "novel_object" ? 1 : 0
       }})

  var noisyPriorOnlyModelPredictions = addNoise(priorOnlyModelPredictions, noise)

  query.add(["modelPrediction","pragmatic","free", row.condition, row.familiarObject, row.subid], [row.age, Math.exp(pragModelPredictions.score(1))])

//  query.add(["modelPrediction","globalPrag","free", row.condition, row.familiarObject, row.subid], [row.age, Math.exp(globalPragModelPredictions.score(1))])

  return {
    prag: pragModelPredictions.score(row.correct),
    noisePrag: noisyPragModelPredictions.score(row.correct),

    globalPrag: globalPragModelPredictions.score(row.correct),
    noiseGlobalPrag: noisyGlobalPragModelPredictions.score(row.correct),

    flat: flatModelPredictions.score(row.correct),
    noiseFlat: noisyFlatModelPredictions.score(row.correct),

    priorOnly: priorOnlyModelPredictions.score(row.correct),
    noisePriorOnly: noisyPriorOnlyModelPredictions.score(row.correct)
  }

}, combData)


var log_likelihood_prag = sum(_.map(all_log_likes, "prag"))
var log_likelihood_prag_noise = sum(_.map(all_log_likes, "noisePrag"))

var log_likelihood_prag_global = sum(_.map(all_log_likes, "globalPrag"))
var log_likelihood_prag_global_noise = sum(_.map(all_log_likes, "noiseGlobalPrag"))

var log_likelihood_flat = sum(_.map(all_log_likes, "flat"))
var log_likelihood_flat_noise = sum(_.map(all_log_likes, "noiseFlat"))

var log_likelihood_prior_only = sum(_.map(all_log_likes, "priorOnly"))
var log_likelihood_prior_only_noise = sum(_.map(all_log_likes, "noisePriorOnly"))

query.add(["log_like", "pragmatics", "parameter_free", "NA", "NA", "NA"], ["NA", log_likelihood_prag])
query.add(["log_like", "pragmatics", "noise", "NA", "NA", "NA"], ["NA", log_likelihood_prag_noise])

query.add(["log_like", "pragmatics_global", "parameter_free", "NA", "NA", "NA"], ["NA", log_likelihood_prag_global])
query.add(["log_like", "pragmatics_global", "noise", "NA", "NA", "NA"], ["NA", log_likelihood_prag_global_noise])

query.add(["log_like", "flat_prior", "parameter_free", "NA", "NA", "NA"], ["NA", log_likelihood_flat])
query.add(["log_like", "flat_prior", "noise", "NA", "NA", "NA"], ["NA", log_likelihood_flat_noise])

query.add(["log_like", "prior_only", "parameter_free", "NA", "NA", "NA"], ["NA", log_likelihood_prior_only])
query.add(["log_like", "prior_only", "noise", "NA", "NA", "NA"], ["NA", log_likelihood_prior_only_noise])


  foreach(function(item){
    var itemLineParameters = all_item_parameters[item]
    query.add(["parameter","items", item, "NA", "NA", "NA"], [itemLineParameters.intercept, itemLineParameters.slope])
  }, familiars)

  query.add(["parameter","parameters","speaker_optimality", "NA", "NA", "NA"], [speakerOptimalityParameters.intercept, speakerOptimalityParameters.slope])
  query.add(["parameter","parameters", "global_sem", "NA", "NA", "NA"], [globalLineParameters.intercept, globalLineParameters.slope])
  query.add(["parameter","sigma", "global_sem_sigmas", "NA", "NA", "NA"], [itemVariability.intercept, itemVariability.slope])
  query.add(["parameter","sigma", "global_subject_sigmas", "NA", "NA", "NA"], ["NA", subject_sigma])

  return query
}


var header = "iteration,a,b,c,d,e,f,g,h"

// var randomNumberForFile = Math.round(uniform(0, 1000))

var totalIterations = 50000, lag =  5;
var samples = totalIterations/lag, burn = totalIterations / 2;
var output_file = 'output/samples_mcmc-'+ totalIterations+'_burn'+burn+'_lag'+lag+'_chain'+chain+'.csv'
var callback = webpplSampleWriter.streamQueryCSV(output_file, header);

var output  = Infer({model,
      samples: samples,
      burn: burn,
      lag: lag,
      method: 'incrementalMH',
      onlyMAP: true,
      verbose: T,
      verboseLag: totalIterations / 20,
      callbacks: [callback]});

'output written to ' + output_file
